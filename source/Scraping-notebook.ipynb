{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3287e9ec",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.852 · Tipología y ciclo de vida de los datos · PRA1</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2022-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PRACTICA 1: Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a4bd0",
   "metadata": {},
   "source": [
    "## 1. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76ff3b",
   "metadata": {},
   "source": [
    "In this first section we are going to describe and explain the main libraries used in the notebook and why we need them:\n",
    "\n",
    "- `requests`:\n",
    "\tPython library that allows the user to send HTTP/1.1 requests easily (POST, GET, PUT, etc.). \n",
    "\tIt is beeing used to get the main content of the URLs used (GET request). \n",
    " \n",
    " \n",
    "- `builtwith`:\n",
    "\tPython library that detects the technology used by a website (Apache, JQuery, Wordpress),\n",
    "\tthe servers and several relevant information.  \n",
    "\tIt will be applied for detecting teh etchnology used for developing the web's design, as it will\n",
    "\tdefine the Web Scraping style that will be necessary to apply.\n",
    "    \n",
    "    \n",
    "- `beautifulsoup4`:\n",
    "\tPython library to scrape information from web pages easily through it HTML or XML file.\n",
    "\tIt is the main library for scraping all the information from the main URL.\n",
    "    \n",
    "    \n",
    "- `python-whois`:\n",
    "\tPython library that produces parsed WHOIS data for a given URL to be able to extract data for all\n",
    "\tthe popular TLDs (com, org, net, …). Also it enables the query of a WHOIS server directly instead of\n",
    "\tgoing through an intermediate web service.\n",
    "\tThis library allows us to know the owner of the webpage we want to scrape and to see if there are any\n",
    "\tscraping blockers.\n",
    "    \n",
    "    \n",
    "- `re`:\n",
    "\tPython library for creating regular expressions to search with.\n",
    "\tIt will helps us to extract the relevant information of the HTML's text.\n",
    "    \n",
    "    \n",
    "- `pandas`:\n",
    "\tPython package that provides fast, flexible, and expressive data structures designed to make working with\n",
    "\t\"relational\" or \"labeled\" data easy and intuitive.\n",
    "\tThis package will be fundamental to develop the final steps of this proyect by creating and exporting the\n",
    "\tfinal dataset.\n",
    " \n",
    " \n",
    "- `session-info`:\n",
    "    Python library that outputs version information for modules loaded in the current session, Python, the OS,\n",
    "    and the CPU.\n",
    "    As extracting a \"requirements.txt\" file using pip freeze would bring too much innecessary data, this library\n",
    "    will solve the problem by just showing the modeles beeing used in this notebook and osme other relevant dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c5d14",
   "metadata": {},
   "source": [
    "### 1.1 Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482c749",
   "metadata": {},
   "source": [
    "If any of the forementioned libraries is not installed in the user's machine, this piece of code will installed them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01488804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\isa31\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\isa31\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\isa31\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\isa31\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\isa31\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: builtwith in c:\\users\\isa31\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: six in c:\\users\\isa31\\anaconda3\\lib\\site-packages (from builtwith) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\isa31\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\isa31\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n",
      "Requirement already satisfied: python-whois in c:\\users\\isa31\\anaconda3\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: future in c:\\users\\isa31\\anaconda3\\lib\\site-packages (from python-whois) (0.18.2)\n",
      "Requirement already satisfied: session-info in c:\\users\\isa31\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: stdlib-list in c:\\users\\isa31\\anaconda3\\lib\\site-packages (from session-info) (0.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install builtwith\n",
    "!pip install beautifulsoup4\n",
    "!pip install python-whois\n",
    "!pip install session-info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af39f16",
   "metadata": {},
   "source": [
    "### 1.2 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16f648",
   "metadata": {},
   "source": [
    "Once the libraries are all installed in the machine, it is time to import them to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0049a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import builtwith\n",
    "import whois\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import session_info\n",
    "\n",
    "# Python file with all the common function used in chapter 3.\n",
    "import utils     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ccb051",
   "metadata": {},
   "source": [
    "## 2. Information about the main URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb89212f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web-servers': ['Nginx'], 'advertising-networks': ['Google AdSense']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the technologies of the webpage we want to scrap\n",
    "builtwith.parse('https://www.expatistan.com/cost-of-living/country/ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50bf6cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the content of the webpage\n",
    "page = requests.get(\"https://www.expatistan.com/cost-of-living/country/ranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998980b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"domain_name\": \"EXPATISTAN.COM\",\n",
      "  \"registrar\": \"NameCheap, Inc.\",\n",
      "  \"whois_server\": \"whois.namecheap.com\",\n",
      "  \"referral_url\": null,\n",
      "  \"updated_date\": \"2018-07-27 13:28:22\",\n",
      "  \"creation_date\": \"2009-08-16 09:25:31\",\n",
      "  \"expiration_date\": \"2023-08-16 09:25:31\",\n",
      "  \"name_servers\": [\n",
      "    \"NS-1210.AWSDNS-23.ORG\",\n",
      "    \"NS-1673.AWSDNS-17.CO.UK\",\n",
      "    \"NS-383.AWSDNS-47.COM\",\n",
      "    \"NS-667.AWSDNS-19.NET\",\n",
      "    \"NS1.LINODE.COM\",\n",
      "    \"NS2.LINODE.COM\",\n",
      "    \"NS3.LINODE.COM\",\n",
      "    \"NS4.LINODE.COM\",\n",
      "    \"NS5.LINODE.COM\"\n",
      "  ],\n",
      "  \"status\": \"clientTransferProhibited https://icann.org/epp#clientTransferProhibited\",\n",
      "  \"emails\": \"abuse@namecheap.com\",\n",
      "  \"dnssec\": \"unsigned\",\n",
      "  \"name\": null,\n",
      "  \"org\": null,\n",
      "  \"address\": null,\n",
      "  \"city\": null,\n",
      "  \"state\": null,\n",
      "  \"registrant_postal_code\": null,\n",
      "  \"country\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Let's see the owner of the domain\n",
    "print(whois.whois(\"expatistan.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08ef7b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/>\n",
      "  <link href=\"https://www.expatistan.com/cost-of-living/country/ranking\" hreflang=\"en\" rel=\"alternate\"/>\n",
      "  <link href=\"https://www.expatistan.com/es/costo-de-vida/pais/ranking\" hreflang=\"es\" rel=\"alternate\"/>\n",
      "  <link href=\"https://www.expatistan.com/pt/custo-de-vida/pais/classificacao\" hreflang=\"pt\" rel=\"alternate\"/>\n",
      "  <title>\n",
      "   Cost of Living Ranking by countries. Updated Apr 2023\n",
      "  </title>\n",
      "  <meta content=\"Ranking of the the cheapest and the most expensives countries in the world with current prices as of Apr 2023.\" name=\"description\"/>\n",
      "  <meta content=\"Cost of Living Ranking by countries. Updated Apr 2023\" property=\"og:title\"/>\n",
      "  <meta content=\"website\" property=\"og:type\"/>\n",
      "  <meta content=\"Ranking of the the cheapest and the most expensives countries in the world with current prices as of Apr 2023.\" property=\"og:description\"/>\n",
      "  <meta content=\"//d2y05869ftj0yg.cloudfront.net/images/map-preview.png\" property=\"og:image\"/>\n",
      "  <meta content=\"https://www.expatistan.com/cost-of-living/country\" property=\"og:url\"/>\n",
      "  <meta content=\"Expatistan, cost of living comparisons\" property=\"og:site_name\"/>\n",
      "  <!--<meta property=\"og:locale\" content=\"en\" />-->\n",
      "  <!--<meta property=\"og:locale:alternate\" content=\"es\" />-->\n",
      "  <meta content=\"502752123,gerardo.robledillo\" property=\"fb:admins\"/>\n",
      "  <meta content=\"145554152152105\" property=\"fb:page_id\"/>\n",
      "  <!--[if lt IE 9]>\n",
      "  <script src=\"//html5shiv.googlecode.com/svn/trunk/html5.js\"></script>\n",
      "  <![endif]-->\n",
      "  <link href=\"https://d2y05869ftj0yg.cloudfront.net\" rel=\"preconnect\"/>\n",
      "  <script>\n",
      "   if (typeof whenDocReady === \"function\") {\n",
      "    // already declared, do nothing\n",
      "  } else {\n",
      "    function whenDocReady(fn) {\n",
      "      // see if DOM is already available\n",
      "      if (document.readyState === \"complete\" || document.readyState === \"interac\n"
     ]
    }
   ],
   "source": [
    "# Let's analyse the structure of the html code and show a bit of it\n",
    "soup = BeautifulSoup(page.content)\n",
    "print(soup.prettify()[:1900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da3b1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "bs4                 4.10.0\n",
       "builtwith           NA\n",
       "pandas              1.3.3\n",
       "requests            2.26.0\n",
       "session_info        1.0.0\n",
       "utils               NA\n",
       "whois               NA\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "anyio                       NA\n",
       "attr                        21.4.0\n",
       "babel                       2.9.1\n",
       "backcall                    0.2.0\n",
       "bottleneck                  1.3.2\n",
       "brotli                      NA\n",
       "certifi                     2021.10.08\n",
       "chardet                     4.0.0\n",
       "charset_normalizer          2.0.4\n",
       "colorama                    0.4.4\n",
       "cython_runtime              NA\n",
       "dateutil                    2.8.2\n",
       "debugpy                     1.4.1\n",
       "decorator                   5.1.0\n",
       "entrypoints                 0.3\n",
       "fastjsonschema              NA\n",
       "future                      0.18.2\n",
       "google                      NA\n",
       "html5lib                    1.1\n",
       "idna                        3.2\n",
       "importlib_resources         NA\n",
       "ipykernel                   6.4.1\n",
       "ipython_genutils            0.2.0\n",
       "jedi                        0.17.1\n",
       "jinja2                      2.11.3\n",
       "json5                       NA\n",
       "jsonschema                  4.5.1\n",
       "jupyter_server              1.4.1\n",
       "jupyterlab_server           2.8.2\n",
       "lxml                        4.6.3\n",
       "markupsafe                  2.0.1\n",
       "mkl                         2.4.0\n",
       "mpl_toolkits                NA\n",
       "nbclassic                   NA\n",
       "nbformat                    5.1.3\n",
       "nt                          NA\n",
       "ntsecuritycon               NA\n",
       "numexpr                     2.7.3\n",
       "numpy                       1.21.2\n",
       "packaging                   21.0\n",
       "parso                       0.8.3\n",
       "past                        0.18.2\n",
       "pickleshare                 0.7.5\n",
       "pkg_resources               NA\n",
       "prometheus_client           NA\n",
       "prompt_toolkit              3.0.20\n",
       "pvectorc                    NA\n",
       "pydev_ipython               NA\n",
       "pydevconsole                NA\n",
       "pydevd                      2.4.1\n",
       "pydevd_concurrency_analyser NA\n",
       "pydevd_file_utils           NA\n",
       "pydevd_plugins              NA\n",
       "pydevd_tracing              NA\n",
       "pygments                    2.10.0\n",
       "pyrsistent                  NA\n",
       "pythoncom                   NA\n",
       "pytz                        2021.3\n",
       "pywintypes                  NA\n",
       "send2trash                  NA\n",
       "six                         1.16.0\n",
       "sniffio                     1.2.0\n",
       "socks                       1.7.1\n",
       "soupsieve                   2.2.1\n",
       "sphinxcontrib               NA\n",
       "storemagic                  NA\n",
       "tornado                     6.1\n",
       "traitlets                   5.1.1\n",
       "urllib3                     1.26.7\n",
       "wcwidth                     0.2.5\n",
       "webencodings                0.5.1\n",
       "win32api                    NA\n",
       "win32com                    NA\n",
       "win32con                    NA\n",
       "win32security               NA\n",
       "win32trace                  NA\n",
       "winerror                    NA\n",
       "zipp                        NA\n",
       "zmq                         22.3.0\n",
       "zope                        NA\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             7.27.0\n",
       "jupyter_client      7.0.1\n",
       "jupyter_core        4.10.0\n",
       "jupyterlab          3.1.7\n",
       "notebook            6.4.3\n",
       "-----\n",
       "Python 3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]\n",
       "Windows-10-10.0.22621-SP0\n",
       "-----\n",
       "Session information updated at 2023-04-16 02:07\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see this sessions dependencies and library versions\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390e4dd",
   "metadata": {},
   "source": [
    "## 3. Web Scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f34d6",
   "metadata": {},
   "source": [
    "### 3.1 Creation of the main class for the Country Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91237ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpatistanCountryScraper():\n",
    "    \"\"\"\n",
    "    Class to carry on the Web Scraping of the Country links from the original URL \n",
    "    www.expatistan.com\n",
    "    ...\n",
    "\n",
    "    Private Methods\n",
    "    ---------------\n",
    "        __init__(url):\n",
    "            Constructor of the class.\n",
    "        __scraping_single_country(self, url, country):\n",
    "            Extract the relevant information of a single Country link from its HTML file\n",
    "        __saving(self):\n",
    "            Creates a Pandas dataframe to save it as a CSV dataset\n",
    "    \n",
    "    Public Methods\n",
    "    --------------\n",
    "        scraping():\n",
    "            Main function of the class that starts the web scraping  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the Expatistan Web Scraper.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.original_url = \"https://www.expatistan.com/cost-of-living/country/ranking\"\n",
    "        \n",
    "        # The dataset, as a first instance, it is going to be stored as a JSON variable\n",
    "        self.dataset =  {\n",
    "            \"Ranking position\": [],\n",
    "            'Country': [],\n",
    "            'Category': [],\n",
    "            'Items': [],\n",
    "            'Original Currency': [],\n",
    "            'Original Currency Value': [],\n",
    "            'Exchanged Currency': [],\n",
    "            'Exchanged Currency Value': []\n",
    "        }\n",
    "         \n",
    "        \n",
    "    def __scraping_single_country(self, url, country):\n",
    "        \"\"\"\n",
    "        Runs the Web Scraping of a particular Country URL\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            link (str): link to the Country's webpage\n",
    "            country (str): name of the Country to be scraped\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # First we need to get the HTML file for this link\n",
    "        country_html = utils.get_HTML(url)\n",
    "        \n",
    "        # Then we get the Ranking position of the Country\n",
    "        pos = utils.get_ranking_pos(country_html, 2)\n",
    "\n",
    "        # All the information is under <table> tag, with class: comparison single-city\n",
    "        table_tags = country_html.find('table', {'class': 'comparison single-city'})\n",
    "\n",
    "        # Setting the common variables for the scraping\n",
    "        current_category = \"\"\n",
    "        current_orig_currency = \"\"\n",
    "        \n",
    "        # For every <tr> tag\n",
    "        for tr in table_tags.findAll(\"tr\"):\n",
    "            # If the tag has the class = \"categoryHeader\"\n",
    "            if \"categoryHeader\" in tr.get(\"class\", []):\n",
    "                # We get the first <th> tag \n",
    "                first_th_tag = tr.find(\"th\")\n",
    "                # And extract its text as the Category of the Dataset's row\n",
    "                current_category = first_th_tag.text\n",
    "                \n",
    "            else:\n",
    "                # If not, it means it has <td> tags and we retrieve them all\n",
    "                td_tags = tr.findAll(\"td\")\n",
    "                \n",
    "                # Depending on how many <td> tags, we scrape different information\n",
    "                # 2 tags means that we can retrive the currency being used in the Country\n",
    "                if len(td_tags) == 2:\n",
    "                    currency = tr.find(\"td\")\n",
    "                    current_orig_currency = currency.text\n",
    "\n",
    "                # 3 tags means we are scraping an european country that uses Euros as currency, \n",
    "                # so the exchange common currency is going to be the same one\n",
    "                elif len(td_tags) == 3: \n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(country)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(\"EUR\")\n",
    "                    self.dataset['Original Currency Value'].append((td_tags[2].text.strip()))\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append((td_tags[2].text.strip()))\n",
    "                \n",
    "                # 4 tags means we are scraping a Country which currency is not Euros,\n",
    "                # so we scrape its own currency and its exchanged value in Euros\n",
    "                elif len(td_tags) == 4:\n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(country)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(current_orig_currency)\n",
    "                    self.dataset['Original Currency Value'].append(td_tags[2].text.strip())\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append(td_tags[3].text.strip())\n",
    "                    \n",
    "        \n",
    "\n",
    "    def scraping(self):\n",
    "        \"\"\"\n",
    "        Main method of the class that starts the Web Scraping of the main webpage\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "\n",
    "        \"\"\"\n",
    "        # 1. Get the original HTML file using Beautiful Soup\n",
    "        html = utils.get_HTML(self.original_url)\n",
    "        \n",
    "        # 2. Get all the Country links to scrape \n",
    "        country_urls = utils.get_links(html, False)\n",
    "        \n",
    "        # 3. For each country, let's scrape all the information\n",
    "        print(\"Starting the Country's Web Scraping!\\n\")\n",
    "        for url in country_urls:\n",
    "            # From the link, we get the last \n",
    "            country = re.search(r\"country/([^/?]*)\", url).group(1)\n",
    "            country = re.sub(\"-\", \" \", country).title()\n",
    "            print(\"Scraping country: \" + country)\n",
    "            self.__scraping_single_country(url, country)\n",
    "\n",
    "        # 4. Finally, we save all the information ina CSV file\n",
    "        print(\"\\nScraping finished!\")\n",
    "        utils.saving(\"../dataset/cost_of_living_countries.csv\", self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204785c",
   "metadata": {},
   "source": [
    "#### 3.1.1 Scraping of Cost of Living per Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78db1640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the Country's Web Scraping!\n",
      "\n",
      "Scraping country: Bermuda\n",
      "Scraping country: Singapore\n",
      "Scraping country: Switzerland\n",
      "Scraping country: Cayman Islands\n",
      "Scraping country: Bahamas\n",
      "Scraping country: Hong Kong\n",
      "Scraping country: Ireland\n",
      "Scraping country: Denmark\n",
      "Scraping country: United States\n",
      "Scraping country: Norway\n",
      "Scraping country: Netherlands\n",
      "Scraping country: Luxembourg\n",
      "Scraping country: Australia\n",
      "Scraping country: New Zealand\n",
      "Scraping country: Qatar\n",
      "Scraping country: United Kingdom\n",
      "Scraping country: Israel\n",
      "Scraping country: Canada\n",
      "Scraping country: Germany\n",
      "Scraping country: Finland\n",
      "Scraping country: Belgium\n",
      "Scraping country: Sweden\n",
      "Scraping country: Austria\n",
      "Scraping country: France\n",
      "Scraping country: Japan\n",
      "Scraping country: United Arab Emirates\n",
      "Scraping country: Malta\n",
      "Scraping country: South Korea\n",
      "Scraping country: Macao\n",
      "Scraping country: Uruguay\n",
      "Scraping country: Italy\n",
      "Scraping country: Panama\n",
      "Scraping country: Dominican Republic\n",
      "Scraping country: Kuwait\n",
      "Scraping country: Palestinian Territory\n",
      "Scraping country: Costa Rica\n",
      "Scraping country: Portugal\n",
      "Scraping country: Bahrain\n",
      "Scraping country: Spain\n",
      "Scraping country: Czech Republic\n",
      "Scraping country: Slovakia\n",
      "Scraping country: Greece\n",
      "Scraping country: China\n",
      "Scraping country: Jordan\n",
      "Scraping country: Chile\n",
      "Scraping country: Taiwan\n",
      "Scraping country: Hungary\n",
      "Scraping country: Thailand\n",
      "Scraping country: Venezuela\n",
      "Scraping country: Poland\n",
      "Scraping country: Serbia\n",
      "Scraping country: Guatemala\n",
      "Scraping country: Bosnia And Herzegovina\n",
      "Scraping country: Mexico\n",
      "Scraping country: Brazil\n",
      "Scraping country: Guyana\n",
      "Scraping country: Cambodia\n",
      "Scraping country: Mauritius\n",
      "Scraping country: Bulgaria\n",
      "Scraping country: Romania\n",
      "Scraping country: Ecuador\n",
      "Scraping country: Vietnam\n",
      "Scraping country: South Africa\n",
      "Scraping country: Fiji\n",
      "Scraping country: Philippines\n",
      "Scraping country: Indonesia\n",
      "Scraping country: Malaysia\n",
      "Scraping country: Russia\n",
      "Scraping country: Peru\n",
      "Scraping country: Kenya\n",
      "Scraping country: Bolivia\n",
      "Scraping country: Colombia\n",
      "Scraping country: Kosovo\n",
      "Scraping country: India\n",
      "\n",
      "Scraping finished!\n",
      "\n",
      "Dataset ../dataset/cost_of_living_countries.csv saved as CSV file!\n"
     ]
    }
   ],
   "source": [
    "scraper = ExpatistanCountryScraper()\n",
    "scraper.scraping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946f8b9",
   "metadata": {},
   "source": [
    "### 3.2 Creation of the main class for the City Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7cc1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpatistanCityScraper():\n",
    "    \"\"\"\n",
    "    Class to carry on the Web Scraping of the cities links from the original URL \n",
    "    www.expatistan.com\n",
    "    ...\n",
    "\n",
    "    Private Methods\n",
    "    ---------------\n",
    "        __init__(url):\n",
    "            Constructor of the class.\n",
    "        __scraping_single_country(self, url, country):\n",
    "            Extract the relevant information of a single Country link from its HTML file\n",
    "        __saving(self):\n",
    "            Creates a Pandas dataframe to save it as a CSV dataset\n",
    "    \n",
    "    Public Methods\n",
    "    --------------\n",
    "        scraping():\n",
    "            Main function of the class that starts the web scraping  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the Expatistan Web Scraper.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.original_url = \"https://www.expatistan.com/cost-of-living/index?ranking=1\"\n",
    "        \n",
    "        # The dataset, as a first instance, it is going to be stored as a JSON variable\n",
    "        self.dataset =  {\n",
    "            \"Ranking position\": [],\n",
    "            'Country': [],\n",
    "            'City': [],\n",
    "            'State': [],\n",
    "            'Category': [],\n",
    "            'Items': [],\n",
    "            'Original Currency': [],\n",
    "            'Original Currency Value': [],\n",
    "            'Exchanged Currency': [],\n",
    "            'Exchanged Currency Value': []\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def __scraping_single_city(self, url):\n",
    "        \"\"\"\n",
    "        Runs the Web Scraping of a particular city URL\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            url (str): link to the city's webpage\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # First we need to get the HTML file for this link\n",
    "        city_html = utils.get_HTML(url)\n",
    "        \n",
    "        # Then we get the Ranking position of the Country\n",
    "        pos = utils.get_ranking_pos(city_html, 3)\n",
    "\n",
    "        # All the information is under <table> tag, with class: comparison single-city\n",
    "        table_tags = city_html.find('table', {'class': 'comparison single-city'})\n",
    "        \n",
    "        \n",
    "        # Setting the common variables for the scraping\n",
    "        current_category = \"\"\n",
    "        current_orig_currency = \"\"\n",
    "        current_country = \"\"\n",
    "        current_city = \"\"\n",
    "        current_state = \"\"\n",
    "        \n",
    "        # Setting the variable city, country and state (if exists)\n",
    "        location = city_html.find('span', {'class': 'city-2'}).text.split(\",\")\n",
    "        current_city = location[0].strip()\n",
    "        if len(location) == 1:\n",
    "            current_state = \"No state\"\n",
    "            current_country = current_city\n",
    "        elif len(location) == 2:\n",
    "            current_state = \"No state\"\n",
    "            current_country = location[1].strip()\n",
    "        else:\n",
    "            current_state = location[1].strip()\n",
    "            current_country = location[2].strip()\n",
    "        \n",
    "        \n",
    "        for tr in table_tags.findAll(\"tr\"):\n",
    "            # If the tag has the class = \"categoryHeader\"\n",
    "            if \"categoryHeader\" in tr.get(\"class\", []):\n",
    "                # We get the first <th> tag \n",
    "                first_th_tag = tr.find(\"th\")\n",
    "                # And extract its text as the Category of the Dataset's row\n",
    "                current_category = first_th_tag.text\n",
    "                \n",
    "            else:\n",
    "                # If not, it means it has <td> tags and we retrieve them all\n",
    "                td_tags = tr.findAll(\"td\")\n",
    "                \n",
    "                # Depending on how many <td> tags, we scrape different information\n",
    "                # 2 tags means that we can retrive the currency being used in the city\n",
    "                if len(td_tags) == 2:\n",
    "                    currency = tr.find(\"td\")\n",
    "                    current_orig_currency = currency.text\n",
    "            \n",
    "                # 3 tags means we are scraping an european city that uses Euros as local currency, \n",
    "                # so the exchange common currency is going to be the same one\n",
    "                elif len(td_tags) == 3: \n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(current_country)\n",
    "                    self.dataset['City'].append(current_city)\n",
    "                    self.dataset['State'].append(current_state)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(\"EUR\")\n",
    "                    self.dataset['Original Currency Value'].append(\"Not defined\"\n",
    "                                                                   if \"-\" in td_tags[2].text.strip()\n",
    "                                                                   else td_tags[2].text.strip())\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append(\"Not defined\"\n",
    "                                                                   if \"-\" in td_tags[2].text.strip()\n",
    "                                                                   else td_tags[2].text.strip())\n",
    "                \n",
    "                # 4 tags means we are scraping a Country which currency is not Euros,\n",
    "                # so we scrape its own currency and its exchanged value in Euros\n",
    "                elif len(td_tags) == 4:\n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(current_country)\n",
    "                    self.dataset['City'].append(current_city)\n",
    "                    self.dataset['State'].append(current_state)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(current_orig_currency)\n",
    "                    self.dataset['Original Currency Value'].append(\"Not defined\"\n",
    "                                                                   if \"-\" in td_tags[2].text.strip()\n",
    "                                                                   else td_tags[2].text.strip())\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append(\"Not defined\"\n",
    "                                                                   if \"-\" in td_tags[3].text.strip()\n",
    "                                                                   else td_tags[3].text.strip())\n",
    "        \n",
    "    def scraping(self):\n",
    "        \"\"\"\n",
    "        Main method of the class that starts the Web Scraping of the main webpage\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "\n",
    "        \"\"\"\n",
    "        # 1. Get the original HTML file using Beautiful Soup\n",
    "        html = utils.get_HTML(self.original_url)\n",
    "        \n",
    "        # 2. Get all the Country links to scrape \n",
    "        cities_urls = utils.get_links(html, True)\n",
    "        \n",
    "        # 3. For each country, let's scrape all the information\n",
    "        print(\"Starting the City's Web Scraping!\\n\")\n",
    "        for url in cities_urls:\n",
    "            # From the link, we get the last \n",
    "            city = re.search(r\"living/([^/?]*)\", url).group(1)\n",
    "            city = re.sub(\"-\", \" \", city).title()\n",
    "            print(\"Scraping city: \" + city)\n",
    "            self.__scraping_single_city(url)\n",
    "\n",
    "        # 4. Finally, we save all the information ina CSV file\n",
    "        print(\"\\nScraping finished!\")\n",
    "        utils.saving(\"../dataset/cost_of_living_cities.csv\", self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ed87c",
   "metadata": {},
   "source": [
    "#### 3.2.1 Scraping of Cost of Living per City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb2884dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the City's Web Scraping!\n",
      "\n",
      "Scraping city: New York City\n",
      "Scraping city: Grand Cayman\n",
      "Scraping city: Singapore\n",
      "Scraping city: Hamilton Bermuda\n",
      "Scraping city: Maui\n",
      "Scraping city: Zurich\n",
      "Scraping city: Geneva\n",
      "Scraping city: Basel\n",
      "Scraping city: Lausanne\n",
      "Scraping city: Miami\n",
      "Scraping city: San Francisco\n",
      "Scraping city: Lugano\n",
      "Scraping city: Oakland California\n",
      "Scraping city: London\n",
      "Scraping city: Los Angeles\n",
      "Scraping city: Washington D C\n",
      "Scraping city: Honolulu\n",
      "Scraping city: Denver\n",
      "Scraping city: Hong Kong\n",
      "Scraping city: Dublin\n",
      "Scraping city: Dallas\n",
      "Scraping city: Seattle\n",
      "Scraping city: San Diego\n",
      "Scraping city: San Jose California\n",
      "Scraping city: Philadelphia\n",
      "Scraping city: Copenhagen\n",
      "Scraping city: Austin\n",
      "Scraping city: Tampa\n",
      "Scraping city: Nassau\n",
      "Scraping city: Vancouver\n",
      "Scraping city: Amsterdam\n",
      "Scraping city: Atlanta\n",
      "Scraping city: Sydney\n",
      "Scraping city: Toronto\n",
      "Scraping city: Raleigh North Carolina\n",
      "Scraping city: Phoenix\n",
      "Scraping city: Sacramento\n",
      "Scraping city: Auckland\n",
      "Scraping city: Chicago\n",
      "Scraping city: Paris\n",
      "Scraping city: Irvine\n",
      "Scraping city: Cocoa Beach Florida\n",
      "Scraping city: Trondheim\n",
      "Scraping city: Minneapolis\n",
      "Scraping city: Melbourne\n",
      "Scraping city: Walnut Creek California\n",
      "Scraping city: Wellington\n",
      "Scraping city: Orlando\n",
      "Scraping city: Tel Aviv\n",
      "Scraping city: Salt Lake City\n",
      "Scraping city: Adelaide\n",
      "Scraping city: Doha\n",
      "Scraping city: Luxembourg\n",
      "Scraping city: Pittsburgh\n",
      "Scraping city: Baltimore\n",
      "Scraping city: Frankfurt Am Main\n",
      "Scraping city: Brisbane\n",
      "Scraping city: The Hague\n",
      "Scraping city: Anchorage\n",
      "Scraping city: Bristol\n",
      "Scraping city: Calgary\n",
      "Scraping city: Newark\n",
      "Scraping city: Stockholm\n",
      "Scraping city: Munich\n",
      "Scraping city: Manchester\n",
      "Scraping city: Brighton And Hove\n",
      "Scraping city: Helsinki\n",
      "Scraping city: Dubai\n",
      "Scraping city: Hamburg\n",
      "Scraping city: Detroit\n",
      "Scraping city: Rotterdam\n",
      "Scraping city: Cork\n",
      "Scraping city: Perth Australia\n",
      "Scraping city: Cologne\n",
      "Scraping city: Edinburgh\n",
      "Scraping city: Ottawa\n",
      "Scraping city: Montreal\n",
      "Scraping city: New Orleans\n",
      "Scraping city: San Antonio Texas\n",
      "Scraping city: Abu Dhabi\n",
      "Scraping city: Nice\n",
      "Scraping city: Oxford\n",
      "Scraping city: Jerusalem\n",
      "Scraping city: Gothenburg\n",
      "Scraping city: Omaha\n",
      "Scraping city: Edmonton\n",
      "Scraping city: Tokyo\n",
      "Scraping city: Aberdeen\n",
      "Scraping city: Brussels\n",
      "Scraping city: Santa Fe New Mexico\n",
      "Scraping city: Lyon\n",
      "Scraping city: Milan\n",
      "Scraping city: Hamilton New Zealand\n",
      "Scraping city: Vienna\n",
      "Scraping city: Cairns\n",
      "Scraping city: Haifa\n",
      "Scraping city: Valletta\n",
      "Scraping city: Plymouth\n",
      "Scraping city: Tucson\n",
      "Scraping city: Newcastle Australia\n",
      "Scraping city: Seoul\n",
      "Scraping city: Barrie\n",
      "Scraping city: San Sebastian\n",
      "Scraping city: Antwerp\n",
      "Scraping city: Rome\n",
      "Scraping city: Portsmouth\n",
      "Scraping city: Stuttgart\n",
      "Scraping city: Wollongong\n",
      "Scraping city: Darwin\n",
      "Scraping city: Modesto\n",
      "Scraping city: Bordeaux\n",
      "Scraping city: Tallahassee\n",
      "Scraping city: Montevideo\n",
      "Scraping city: Macao\n",
      "Scraping city: Marseille\n",
      "Scraping city: Stoke On Trent\n",
      "Scraping city: Madrid\n",
      "Scraping city: Winnipeg\n",
      "Scraping city: Ramallah\n",
      "Scraping city: Barcelona Spain\n",
      "Scraping city: Osaka\n",
      "Scraping city: Innsbruck\n",
      "Scraping city: Florence\n",
      "Scraping city: Nantes\n",
      "Scraping city: Montpellier\n",
      "Scraping city: Lille\n",
      "Scraping city: Lisbon\n",
      "Scraping city: Ghent\n",
      "Scraping city: Panama City\n",
      "Scraping city: Abbotsford\n",
      "Scraping city: Essen\n",
      "Scraping city: Dortmund\n",
      "Scraping city: Santo Domingo\n",
      "Scraping city: Freiburg\n",
      "Scraping city: Venice\n",
      "Scraping city: Varese\n",
      "Scraping city: Quebec City\n",
      "Scraping city: Athens Greece\n",
      "Scraping city: Bilbao\n",
      "Scraping city: San Jose Costa Rica\n",
      "Scraping city: Bologna\n",
      "Scraping city: Prague\n",
      "Scraping city: Kuwait\n",
      "Scraping city: Shanghai\n",
      "Scraping city: Pamplona\n",
      "Scraping city: Valencia Spain\n",
      "Scraping city: Manama\n",
      "Scraping city: Youngstown Ohio\n",
      "Scraping city: Perugia\n",
      "Scraping city: Malaga\n",
      "Scraping city: Dresden\n",
      "Scraping city: Oporto\n",
      "Scraping city: San Antonio California\n",
      "Scraping city: Santiago Chile\n",
      "Scraping city: Taipei\n",
      "Scraping city: Naples\n",
      "Scraping city: Pisa\n",
      "Scraping city: Pescara\n",
      "Scraping city: Zaragoza\n",
      "Scraping city: Oviedo\n",
      "Scraping city: Turin\n",
      "Scraping city: Santa Cruz De Tenerife\n",
      "Scraping city: Amman\n",
      "Scraping city: Cedar Rapids\n",
      "Scraping city: Corunna\n",
      "Scraping city: Monterrey\n",
      "Scraping city: Alicante\n",
      "Scraping city: Beijing\n",
      "Scraping city: Santos Sao Paulo\n",
      "Scraping city: Las Palmas De Gran Canaria\n",
      "Scraping city: Seville\n",
      "Scraping city: Budapest\n",
      "Scraping city: Funchal\n",
      "Scraping city: Sao Paulo\n",
      "Scraping city: Mexico City\n",
      "Scraping city: Moscow\n",
      "Scraping city: Shenzhen\n",
      "Scraping city: Caracas\n",
      "Scraping city: Bangkok\n",
      "Scraping city: Warsaw\n",
      "Scraping city: Bratislava\n",
      "Scraping city: New Guatemala\n",
      "Scraping city: Almeria\n",
      "Scraping city: Barquisimeto\n",
      "Scraping city: Brasilia\n",
      "Scraping city: Belgrade\n",
      "Scraping city: Florianopolis\n",
      "Scraping city: Rio De Janeiro\n",
      "Scraping city: Belo Horizonte\n",
      "Scraping city: Port Louis\n",
      "Scraping city: Porto Alegre\n",
      "Scraping city: Quito\n",
      "Scraping city: Guadalajara\n",
      "Scraping city: Georgetown Guyana\n",
      "Scraping city: Bucharest\n",
      "Scraping city: Suva Fiji\n",
      "Scraping city: Queretaro\n",
      "Scraping city: Johannesburg\n",
      "Scraping city: Guayaquil\n",
      "Scraping city: Manila\n",
      "Scraping city: Lima\n",
      "Scraping city: Phnom Penh\n",
      "Scraping city: Vitoria Brazil\n",
      "Scraping city: Jakarta\n",
      "Scraping city: Chengdu\n",
      "Scraping city: Kuala Lumpur\n",
      "Scraping city: Nairobi\n",
      "Scraping city: Ho Chi Minh City\n",
      "Scraping city: Johor Bahru\n",
      "Scraping city: Bogota\n",
      "Scraping city: La Paz Bolivia\n",
      "Scraping city: Medellin\n",
      "Scraping city: Yakutsk\n",
      "Scraping city: Plovdiv\n",
      "Scraping city: Iasi\n",
      "Scraping city: Mostar\n",
      "Scraping city: Mumbai\n",
      "Scraping city: Bandung\n",
      "Scraping city: Burgas\n",
      "Scraping city: Santiago De Cali\n",
      "Scraping city: Zenica\n",
      "Scraping city: Pristina\n",
      "Scraping city: Bangalore\n",
      "Scraping city: Targu Mures\n",
      "Scraping city: Pune\n",
      "Scraping city: Delhi\n",
      "Scraping city: Yogyakarta\n",
      "Scraping city: Nagpur\n",
      "Scraping city: Irkutsk\n",
      "\n",
      "Scraping finished!\n",
      "\n",
      "Dataset ../dataset/cost_of_living_cities.csv saved as CSV file!\n"
     ]
    }
   ],
   "source": [
    "scraper = ExpatistanCityScraper()\n",
    "scraper.scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7703bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
