{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3287e9ec",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.852 · Tipología y ciclo de vida de los datos · PRA1</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2022-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PRA1: Web scraping - Testing file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a4bd0",
   "metadata": {},
   "source": [
    "## 1. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76ff3b",
   "metadata": {},
   "source": [
    "In this first section we are going to describe and explain the main libraries used in the notebook and why we need them:\n",
    "\n",
    "- `requests`:\n",
    "\tPython library that allows the user to send HTTP/1.1 requests easily (POST, GET, PUT, etc.). \n",
    "\tIt is beeing used to get the main content of the URLs used (GET request). \n",
    " \n",
    " \n",
    "- `builtwith`:\n",
    "\tPython library that detects the technology used by a website (Apache, JQuery, Wordpress),\n",
    "\tthe servers and several relevant information.  \n",
    "\tIt will be applied for detecting teh etchnology used for developing the web's design, as it will\n",
    "\tdefine the Web Scraping style that will be necessary to apply.\n",
    "    \n",
    "    \n",
    "- `beautifulsoup4`:\n",
    "\tPython library to scrape information from web pages easily through it HTML or XML file.\n",
    "\tIt is the main library for scraping all the information from the main URL.\n",
    "    \n",
    "    \n",
    "- `python-whois`:\n",
    "\tPython library that produces parsed WHOIS data for a given URL to be able to extract data for all\n",
    "\tthe popular TLDs (com, org, net, …). Also it enables the query of a WHOIS server directly instead of\n",
    "\tgoing through an intermediate web service.\n",
    "\tThis library allows us to know the owner of the webpage we want to scrape and to see if there are any\n",
    "\tscraping blockers.\n",
    "    \n",
    "    \n",
    "- `re`:\n",
    "\tPython library for creating regular expressions to search with.\n",
    "\tIt will helps us to extract the relevant information of the HTML's text.\n",
    "    \n",
    "    \n",
    "- `pandas`:\n",
    "\tPython package that provides fast, flexible, and expressive data structures designed to make working with\n",
    "\t\"relational\" or \"labeled\" data easy and intuitive.\n",
    "\tThis package will be fundamental to develop the final steps of this proyect by creating and exporting the\n",
    "\tfinal dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c5d14",
   "metadata": {},
   "source": [
    "### 1.1 Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482c749",
   "metadata": {},
   "source": [
    "If any of the forementioned libraries is not installed in teh user's machine, this piece of code will installed them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01488804",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install builtwith\n",
    "!pip install beautifulsoup4\n",
    "!pip install python-whois"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af39f16",
   "metadata": {},
   "source": [
    "### 1.2 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16f648",
   "metadata": {},
   "source": [
    "Once the libraries are all installed in the machine, it is time to import them to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0049a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import builtwith\n",
    "import whois\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ccb051",
   "metadata": {},
   "source": [
    "## 2. Information about the main URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the technologies of the webpage we want to scrap\n",
    "builtwith.parse('https://www.expatistan.com/cost-of-living/country/ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf6cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the content of the webpage\n",
    "page = requests.get(\"https://www.expatistan.com/cost-of-living/country/ranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998980b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's see the owner of the domain\n",
    "print(whois.whois(\"expatistan.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef7b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's analyse the structure of the html code\n",
    "soup = BeautifulSoup(page.content)\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390e4dd",
   "metadata": {},
   "source": [
    "## 3. Web Scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f34d6",
   "metadata": {},
   "source": [
    "### 3.1 Creation of the main classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91237ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpatistanScraper():\n",
    "    \"\"\"\n",
    "    Class to carry on the Web Scraping of the Country links from the original URL \n",
    "    www.expatistan.com\n",
    "    ...\n",
    "\n",
    "    Private Methods\n",
    "    ---------------\n",
    "        __init__(url):\n",
    "            Constructor of the class.\n",
    "        __get_HTML(url):\n",
    "            Returns the HTML code of the given url\n",
    "        __get_countries_links(html):\n",
    "            Retruns all teh available links related to the Countries in the main page\n",
    "        __get_ranking_pos(html):\n",
    "            Return teh ranking position of a specific Country in the Countries' Ranking main page\n",
    "        __scraping_single_country(self, url, country):\n",
    "            Extract the relevant information of a single Country link from its HTML file\n",
    "        __saving(self):\n",
    "            Creates a Pandas dataframe to save it as a CSV dataset\n",
    "    \n",
    "    Public Methods\n",
    "    --------------\n",
    "        scraping():\n",
    "            Main function of the class that starts the web scraping  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the Expatistan Web Scraper.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.original_url = \"https://www.expatistan.com/cost-of-living/country/ranking\"\n",
    "        \n",
    "        # The dataset, as a first instance, it is going to be stored as a JSON variable\n",
    "        self.dataset =  {\n",
    "            \"Ranking position\": [],\n",
    "            'Country': [],\n",
    "            'Category': [],\n",
    "            'Items': [],\n",
    "            'Original Currency': [],\n",
    "            'Original Currency Value': [],\n",
    "            'Exchanged Currency': [],\n",
    "            'Exchanged Currency Value': []\n",
    "        }\n",
    "         \n",
    "            \n",
    "    def __get_HTML(self, url):\n",
    "        '''\n",
    "        Returns the HTML file of a webpage by its link\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            url (str): link of the webpage\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            HTML file of the webpage\n",
    "        '''\n",
    "        # First, we need to request the contents of the webpage\n",
    "        page = requests.get(url)\n",
    "        \n",
    "        # Then we return its HTML file\n",
    "        return BeautifulSoup(page.content)\n",
    "        \n",
    "        \n",
    "    def __get_countries_links(self, html):\n",
    "        \"\"\"\n",
    "        Returns a list of the links to be scraped from the main webpage\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            html (str): HTML code of the original page\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            List of string values which are the links to the Countries' pages\n",
    "\n",
    "        \"\"\"\n",
    "        # From the HTML file, we collect all the <td> tags that have a class = \"country-name\"\n",
    "        td_tags = html.find_all('td', {\"class\": \"country-name\"})\n",
    "        \n",
    "        # Then, for every <td> tag, we get the hyperlink in its <a> tag and add the extra currency reference\n",
    "        countries_links = [td.find('a').get('href') + \"?currency=EUR\" for td in td_tags]\n",
    "        return countries_links\n",
    "    \n",
    "    \n",
    "    def __get_ranking_pos(self, html):\n",
    "        \"\"\"\n",
    "        Returns the ranking position of a particular Country\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            html (str): HTML code of the Country's page\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Number of the position in string format\n",
    "\n",
    "        \"\"\"\n",
    "        # First, we get all <li> tags that have a class = \"key-point\"\n",
    "        li_tags = html.findAll('li', {'class': 'key-point'})\n",
    "        \n",
    "        # For every <li> tag\n",
    "        for li in li_tags:\n",
    "            # If the word \"World\" is in its text\n",
    "            if 'World' in li.text:\n",
    "                # We split the whole text by the word \"World\", keeping the side after it\n",
    "                text = li.text.split(\"World\", 1)[1].strip()\n",
    "                # From the remaining text we get the first 1 or 2 digits that appear\n",
    "                pos = re.search(r'\\b(\\d{1,2})\\b', text).group(1)\n",
    "                \n",
    "        return pos\n",
    "\n",
    "    \n",
    "    def scraping(self):\n",
    "        \"\"\"\n",
    "        Main method of the class that starts the Web Scraping of the main webpage\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "\n",
    "        \"\"\"\n",
    "        # 1. Get the original HTML file using Beautiful Soup\n",
    "        html = self.__get_HTML(self.original_url)\n",
    "        \n",
    "        # 2. Get all the Country links to scrape \n",
    "        country_urls = self.__get_countries_links(html)\n",
    "        \n",
    "        # 3. For each country, let's scrape all the information\n",
    "        for url in country_urls:\n",
    "            # From the link, we get the last \n",
    "            country = re.search(r\"country/([^/?]*)\", url).group(1)\n",
    "            country = re.sub(\"-\", \" \", country).title()\n",
    "            #print(\"Scraping country \" + country)\n",
    "            self.__scraping_single_country(url, country)\n",
    "\n",
    "        # 4. Finally, we save all the information ina CSV file\n",
    "        print(\"\\nScraping finished!\")\n",
    "        self.__saving()\n",
    "            \n",
    "        \n",
    "    def __scraping_single_country(self, url, country):\n",
    "        \"\"\"\n",
    "        Runs the Web Scraping of a particular Country URL\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            link (str): link to the Country's webpage\n",
    "            country (str): name of the Country to be scraped\n",
    "\n",
    "        \"\"\"\n",
    "        # First we need to get the HTML file for this link\n",
    "        country_html = self.__get_HTML(url)\n",
    "        \n",
    "        # Then we get the Ranking position of the Country\n",
    "        pos = self.__get_ranking_pos(country_html)\n",
    "\n",
    "        # All the information is under <tr> tags, let's find them all\n",
    "        tr_tags = country_html.findAll('tr')\n",
    "\n",
    "        # But not all of them have value to the proyect, so we need to remove them\n",
    "        # The last one regards setting of the view, not information\n",
    "        tr_tags.pop(-1)\n",
    "\n",
    "        # For every <tr> asnd its position in the list\n",
    "        for i, tr in enumerate(tr_tags):\n",
    "            # We get the first <td> and/or <th> tag that the <tr> tag holds\n",
    "            td = tr.find(\"td\")\n",
    "            th = tr.find(\"th\")\n",
    "\n",
    "            # Then we check if its one of the non-valuable types of label by its attributes\n",
    "            # If so, we get rid of it\n",
    "            if td and td.get(\"colspan\") == \"4\":\n",
    "                tr_tags.pop(i)\n",
    "            elif th and th.get(\"class\") == [\"ranking\"]:\n",
    "                tr_tags.pop(i)\n",
    "                \n",
    "            elif td:\n",
    "                # However some of the <td> tags have hyperlink tags, <a>, inside to city pages\n",
    "                # so we need to track them by searching on the first one if it has an <a> tag\n",
    "                tds = tr.findAll(\"td\")\n",
    "                first_td_tag = tds[0]\n",
    "                \n",
    "                # Verify if the <td> tag has the <a> tag\n",
    "                if first_td_tag.find(\"a\") is not None:\n",
    "                    # If so, erase it\n",
    "                    tr_tags.pop(i)\n",
    "               \n",
    "                    \n",
    "                    \n",
    "                \n",
    "\n",
    "        # Setting the common variables for the scraping\n",
    "        current_category = \"\"\n",
    "        current_orig_currency = \"\"\n",
    "        \n",
    "        # For every <tr> tag\n",
    "        for tr in tr_tags:\n",
    "            # If the tag has the class = \"categoryHeader\"\n",
    "            if \"categoryHeader\" in tr.get(\"class\", []):\n",
    "                # We get the first <th> tag \n",
    "                first_th_tag = tr.find(\"th\")\n",
    "                # And extract its text as the Category of the Dataset's row\n",
    "                current_category = first_th_tag.text\n",
    "                \n",
    "            else:\n",
    "                # If not, it means it has <td> tags and we retrieve them all\n",
    "                td_tags = tr.findAll(\"td\")\n",
    "                \n",
    "                # Depending on how many <td> tags, we scrape different information\n",
    "                # 2 tags means that we can retrive the currency being used in the Country\n",
    "                if len(td_tags) == 2:\n",
    "                    currency = tr.find(\"td\")\n",
    "                    current_orig_currency = currency.text\n",
    "\n",
    "                # 3 tags means we are scraping an european country that uses Euros as currency, \n",
    "                # so the exchange common currency is going to be the same one\n",
    "                elif len(td_tags) == 3: \n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(country)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(\"EUR\")\n",
    "                    self.dataset['Original Currency Value'].append((td_tags[2].text.strip()))\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append((td_tags[2].text.strip()))\n",
    "                \n",
    "                # 4 tags means we are scraping a Country which currency is not Euros,\n",
    "                # so we scrape its own currency and its exchanged value in Euros\n",
    "                elif len(td_tags) == 4:\n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(country)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(current_orig_currency)\n",
    "                    self.dataset['Original Currency Value'].append((td_tags[2].text.strip()))\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append((td_tags[3].text.strip()))\n",
    "                    \n",
    "            \n",
    "    def __saving(self):\n",
    "        \"\"\"\n",
    "        Creates a Pandas Dataframe from the scraped data to save it as a CSV dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "\n",
    "        \"\"\"\n",
    "        # Let's create a Pandas Dataframe with the obtained dataset\n",
    "        expatistan_df = pd.DataFrame(self.dataset)\n",
    "        \n",
    "        # Now we save it as a CSV file with no index column\n",
    "        expatistan_df.to_csv(\"expatistan.csv\", index = False)\n",
    "        \n",
    "        print(\"\\nDataset saved as CSV file!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = ExpatistanScraper()\n",
    "scraper.scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6088a919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
