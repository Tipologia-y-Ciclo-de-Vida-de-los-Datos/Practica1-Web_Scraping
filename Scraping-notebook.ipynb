{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3287e9ec",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.852 · Tipología y ciclo de vida de los datos · PRA1</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2022-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PRA1: Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a4bd0",
   "metadata": {},
   "source": [
    "## 1. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76ff3b",
   "metadata": {},
   "source": [
    "In this first section we are going to describe and explain the main libraries used in the notebook and why we need them:\n",
    "\n",
    "- `requests`:\n",
    "\tPython library that allows the user to send HTTP/1.1 requests easily (POST, GET, PUT, etc.). \n",
    "\tIt is beeing used to get the main content of the URLs used (GET request). \n",
    " \n",
    " \n",
    "- `builtwith`:\n",
    "\tPython library that detects the technology used by a website (Apache, JQuery, Wordpress),\n",
    "\tthe servers and several relevant information.  \n",
    "\tIt will be applied for detecting teh etchnology used for developing the web's design, as it will\n",
    "\tdefine the Web Scraping style that will be necessary to apply.\n",
    "    \n",
    "    \n",
    "- `beautifulsoup4`:\n",
    "\tPython library to scrape information from web pages easily through it HTML or XML file.\n",
    "\tIt is the main library for scraping all the information from the main URL.\n",
    "    \n",
    "    \n",
    "- `python-whois`:\n",
    "\tPython library that produces parsed WHOIS data for a given URL to be able to extract data for all\n",
    "\tthe popular TLDs (com, org, net, …). Also it enables the query of a WHOIS server directly instead of\n",
    "\tgoing through an intermediate web service.\n",
    "\tThis library allows us to know the owner of the webpage we want to scrape and to see if there are any\n",
    "\tscraping blockers.\n",
    "    \n",
    "    \n",
    "- `re`:\n",
    "\tPython library for creating regular expressions to search with.\n",
    "\tIt will helps us to extract the relevant information of the HTML's text.\n",
    "    \n",
    "    \n",
    "- `pandas`:\n",
    "\tPython package that provides fast, flexible, and expressive data structures designed to make working with\n",
    "\t\"relational\" or \"labeled\" data easy and intuitive.\n",
    "\tThis package will be fundamental to develop the final steps of this proyect by creating and exporting the\n",
    "\tfinal dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c5d14",
   "metadata": {},
   "source": [
    "### 1.1 Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482c749",
   "metadata": {},
   "source": [
    "If any of the forementioned libraries is not installed in the user's machine, this piece of code will installed them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01488804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/jose/opt/anaconda3/lib/python3.9/site-packages (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jose/opt/anaconda3/lib/python3.9/site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jose/opt/anaconda3/lib/python3.9/site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jose/opt/anaconda3/lib/python3.9/site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/jose/opt/anaconda3/lib/python3.9/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: builtwith in /Users/jose/opt/anaconda3/lib/python3.9/site-packages (1.3.4)\n",
      "Requirement already satisfied: six in /Users/jose/opt/anaconda3/lib/python3.9/site-packages (from builtwith) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/jose/opt/anaconda3/lib/python3.9/site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jose/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.2.1)\n",
      "Requirement already satisfied: python-whois in /Users/jose/opt/anaconda3/lib/python3.9/site-packages (0.8.0)\n",
      "Requirement already satisfied: future in /Users/jose/opt/anaconda3/lib/python3.9/site-packages (from python-whois) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install builtwith\n",
    "!pip install beautifulsoup4\n",
    "!pip install python-whois"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af39f16",
   "metadata": {},
   "source": [
    "### 1.2 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16f648",
   "metadata": {},
   "source": [
    "Once the libraries are all installed in the machine, it is time to import them to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0049a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import builtwith\n",
    "import whois\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ccb051",
   "metadata": {},
   "source": [
    "## 2. Information about the main URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the technologies of the webpage we want to scrap\n",
    "builtwith.parse('https://www.expatistan.com/cost-of-living/country/ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf6cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the content of the webpage\n",
    "page = requests.get(\"https://www.expatistan.com/cost-of-living/country/ranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998980b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's see the owner of the domain\n",
    "print(whois.whois(\"expatistan.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef7b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's analyse the structure of the html code\n",
    "soup = BeautifulSoup(page.content)\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390e4dd",
   "metadata": {},
   "source": [
    "## 3. Web Scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f34d6",
   "metadata": {},
   "source": [
    "### 3.1 Creation of the main classes for countries info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91237ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpatistanCountryScraper():\n",
    "    \"\"\"\n",
    "    Class to carry on the Web Scraping of the Country links from the original URL \n",
    "    www.expatistan.com\n",
    "    ...\n",
    "\n",
    "    Private Methods\n",
    "    ---------------\n",
    "        __init__(url):\n",
    "            Constructor of the class.\n",
    "        __get_HTML(url):\n",
    "            Returns the HTML code of the given url\n",
    "        __get_countries_links(html):\n",
    "            Retruns all the available links related to the Countries in the main page\n",
    "        __get_ranking_pos(html):\n",
    "            Return the ranking position of a specific Country in the Countries' Ranking main page\n",
    "        __scraping_single_country(self, url, country):\n",
    "            Extract the relevant information of a single Country link from its HTML file\n",
    "        __saving(self):\n",
    "            Creates a Pandas dataframe to save it as a CSV dataset\n",
    "    \n",
    "    Public Methods\n",
    "    --------------\n",
    "        scraping():\n",
    "            Main function of the class that starts the web scraping  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the Expatistan Web Scraper.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.original_url = \"https://www.expatistan.com/cost-of-living/country/ranking\"\n",
    "        \n",
    "        # The dataset, as a first instance, it is going to be stored as a JSON variable\n",
    "        self.dataset =  {\n",
    "            \"Ranking position\": [],\n",
    "            'Country': [],\n",
    "            'Category': [],\n",
    "            'Items': [],\n",
    "            'Original Currency': [],\n",
    "            'Original Currency Value': [],\n",
    "            'Exchanged Currency': [],\n",
    "            'Exchanged Currency Value': []\n",
    "        }\n",
    "         \n",
    "            \n",
    "    def __get_HTML(self, url):\n",
    "        '''\n",
    "        Returns the HTML file of a webpage by its link\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            url (str): link of the webpage\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            HTML file of the webpage\n",
    "        '''\n",
    "        # First, we need to request the contents of the webpage\n",
    "        page = requests.get(url)\n",
    "        \n",
    "        # Then we return its HTML file\n",
    "        return BeautifulSoup(page.content)\n",
    "        \n",
    "        \n",
    "    def __get_countries_links(self, html):\n",
    "        \"\"\"\n",
    "        Returns a list of the links to be scraped from the main webpage\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            html (str): HTML code of the original page\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            List of string values which are the links to the Countries' pages\n",
    "\n",
    "        \"\"\"\n",
    "        # From the HTML file, we collect all the <td> tags that have a class = \"country-name\"\n",
    "        td_tags = html.find_all('td', {\"class\": \"country-name\"})\n",
    "        \n",
    "        # Then, for every <td> tag, we get the hyperlink in its <a> tag and add the extra currency reference\n",
    "        countries_links = [td.find('a').get('href') + \"?currency=EUR\" for td in td_tags]\n",
    "        return countries_links\n",
    "    \n",
    "    \n",
    "    def __get_ranking_pos(self, html):\n",
    "        \"\"\"\n",
    "        Returns the ranking position of a particular Country\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            html (str): HTML code of the Country's page\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Number of the position in string format\n",
    "\n",
    "        \"\"\"\n",
    "        # First, we get all <li> tags that have a class = \"key-point\"\n",
    "        li_tags = html.findAll('li', {'class': 'key-point'})\n",
    "        \n",
    "        # For every <li> tag\n",
    "        for li in li_tags:\n",
    "            # If the word \"World\" is in its text\n",
    "            if 'World' in li.text:\n",
    "                # We split the whole text by the word \"World\", keeping the side after it\n",
    "                text = li.text.split(\"World\", 1)[1].strip()\n",
    "                # From the remaining text we get the first 1 or 2 digits that appear\n",
    "                pos = re.search(r'\\b(\\d{1,2})\\b', text).group(1)\n",
    "                \n",
    "        return pos\n",
    "            \n",
    "        \n",
    "    def __scraping_single_country(self, url, country):\n",
    "        \"\"\"\n",
    "        Runs the Web Scraping of a particular Country URL\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            link (str): link to the Country's webpage\n",
    "            country (str): name of the Country to be scraped\n",
    "\n",
    "        \"\"\"\n",
    "        # First we need to get the HTML file for this link\n",
    "        country_html = self.__get_HTML(url)\n",
    "        \n",
    "        # Then we get the Ranking position of the Country\n",
    "        pos = self.__get_ranking_pos(country_html)\n",
    "\n",
    "        # All the information is under <tr> tags, let's find them all\n",
    "        tr_tags = country_html.findAll('tr')\n",
    "\n",
    "        # But not all of them have value to the proyect, so we need to remove them\n",
    "        # The last one regards setting of the view, not information\n",
    "        tr_tags.pop(-1)\n",
    "\n",
    "        # For every <tr> asnd its position in the list\n",
    "        for i, tr in enumerate(tr_tags):\n",
    "            # We get the first <td> and/or <th> tag that the <tr> tag holds\n",
    "            td = tr.find(\"td\")\n",
    "            th = tr.find(\"th\")\n",
    "\n",
    "            # Then we check if its one of the non-valuable types of label by its attributes\n",
    "            # If so, we get rid of it\n",
    "            if td and td.get(\"colspan\") == \"4\":\n",
    "                tr_tags.pop(i)\n",
    "            elif th and th.get(\"class\") == [\"ranking\"]:\n",
    "                tr_tags.pop(i)\n",
    "                \n",
    "            elif td:\n",
    "                # However some of the <td> tags have hyperlink tags, <a>, inside to city pages\n",
    "                # so we need to track them by searching on the first one if it has an <a> tag\n",
    "                tds = tr.findAll(\"td\")\n",
    "                first_td_tag = tds[0]\n",
    "                \n",
    "                # Verify if the <td> tag has the <a> tag\n",
    "                if first_td_tag.find(\"a\") is not None:\n",
    "                    # If so, erase it\n",
    "                    tr_tags.pop(i)\n",
    "               \n",
    "             \n",
    "\n",
    "        # Setting the common variables for the scraping\n",
    "        current_category = \"\"\n",
    "        current_orig_currency = \"\"\n",
    "        \n",
    "        # For every <tr> tag\n",
    "        for tr in tr_tags:\n",
    "            # If the tag has the class = \"categoryHeader\"\n",
    "            if \"categoryHeader\" in tr.get(\"class\", []):\n",
    "                # We get the first <th> tag \n",
    "                first_th_tag = tr.find(\"th\")\n",
    "                # And extract its text as the Category of the Dataset's row\n",
    "                current_category = first_th_tag.text\n",
    "                \n",
    "            else:\n",
    "                # If not, it means it has <td> tags and we retrieve them all\n",
    "                td_tags = tr.findAll(\"td\")\n",
    "                \n",
    "                # Depending on how many <td> tags, we scrape different information\n",
    "                # 2 tags means that we can retrive the currency being used in the Country\n",
    "                if len(td_tags) == 2:\n",
    "                    currency = tr.find(\"td\")\n",
    "                    current_orig_currency = currency.text\n",
    "\n",
    "                # 3 tags means we are scraping an european country that uses Euros as currency, \n",
    "                # so the exchange common currency is going to be the same one\n",
    "                elif len(td_tags) == 3: \n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(country)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(\"EUR\")\n",
    "                    self.dataset['Original Currency Value'].append((td_tags[2].text.strip()))\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append((td_tags[2].text.strip()))\n",
    "                \n",
    "                # 4 tags means we are scraping a Country which currency is not Euros,\n",
    "                # so we scrape its own currency and its exchanged value in Euros\n",
    "                elif len(td_tags) == 4:\n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(country)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(current_orig_currency)\n",
    "                    self.dataset['Original Currency Value'].append((td_tags[2].text.strip()))\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append((td_tags[3].text.strip()))\n",
    "                    \n",
    "            \n",
    "    def __saving(self):\n",
    "        \"\"\"\n",
    "        Creates a Pandas Dataframe from the scraped data to save it as a CSV dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "\n",
    "        \"\"\"\n",
    "        # Let's create a Pandas Dataframe with the obtained dataset\n",
    "        expatistan_df = pd.DataFrame(self.dataset)\n",
    "        \n",
    "        # Now we save it as a CSV file with no index column\n",
    "        expatistan_df.to_csv(\"expatistan.csv\", index = False)\n",
    "        \n",
    "        print(\"\\nDataset saved as CSV file!\")\n",
    "        \n",
    "\n",
    "    def scraping(self):\n",
    "        \"\"\"\n",
    "        Main method of the class that starts the Web Scraping of the main webpage\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "\n",
    "        \"\"\"\n",
    "        # 1. Get the original HTML file using Beautiful Soup\n",
    "        html = self.__get_HTML(self.original_url)\n",
    "        \n",
    "        # 2. Get all the Country links to scrape \n",
    "        country_urls = self.__get_countries_links(html)\n",
    "        \n",
    "        # 3. For each country, let's scrape all the information\n",
    "        for url in country_urls:\n",
    "            # From the link, we get the last \n",
    "            country = re.search(r\"country/([^/?]*)\", url).group(1)\n",
    "            country = re.sub(\"-\", \" \", country).title()\n",
    "            #print(\"Scraping country \" + country)\n",
    "            self.__scraping_single_country(url, country)\n",
    "\n",
    "        # 4. Finally, we save all the information ina CSV file\n",
    "        print(\"\\nScraping finished!\")\n",
    "        self.__saving()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = ExpatistanCountryScraper()\n",
    "scraper.scraping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946f8b9",
   "metadata": {},
   "source": [
    "## CIUDADES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7cc1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpatistanCityScraper():\n",
    "    \"\"\"\n",
    "    Class to carry on the Web Scraping of the cities links from the original URL \n",
    "    www.expatistan.com\n",
    "    ...\n",
    "\n",
    "    Private Methods\n",
    "    ---------------\n",
    "        __init__(url):\n",
    "            Constructor of the class.\n",
    "        __get_HTML(url):\n",
    "            Returns the HTML code of the given url\n",
    "        __get_cities_links(html):\n",
    "            Returns all the available links related to the cities in the main page\n",
    "        __get_ranking_pos(html):\n",
    "            Return the ranking position of a specific city in the cities' Ranking main page\n",
    "        __scraping_single_country(self, url, country):\n",
    "            Extract the relevant information of a single Country link from its HTML file\n",
    "        __saving(self):\n",
    "            Creates a Pandas dataframe to save it as a CSV dataset\n",
    "    \n",
    "    Public Methods\n",
    "    --------------\n",
    "        scraping():\n",
    "            Main function of the class that starts the web scraping  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the Expatistan Web Scraper.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.original_url = \"https://www.expatistan.com/cost-of-living/index?ranking=1\"\n",
    "        \n",
    "        # The dataset, as a first instance, it is going to be stored as a JSON variable\n",
    "        self.dataset =  {\n",
    "            \"Ranking position\": [],\n",
    "            'Country': [],\n",
    "            'City': [],\n",
    "            'State': [],\n",
    "            'Category': [],\n",
    "            'Items': [],\n",
    "            'Original Currency': [],\n",
    "            'Original Currency Value': [],\n",
    "            'Exchanged Currency': [],\n",
    "            'Exchanged Currency Value': []\n",
    "        }\n",
    "         \n",
    "            \n",
    "    def __get_HTML(self, url):\n",
    "        '''\n",
    "        Returns the HTML file of a webpage by its link\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            url (str): link of the webpage\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            HTML file of the webpage\n",
    "        '''\n",
    "        # First, we need to request the contents of the webpage\n",
    "        page = requests.get(url)\n",
    "        \n",
    "        # Then we return its HTML file\n",
    "        return BeautifulSoup(page.content)\n",
    "        \n",
    "        \n",
    "    def __get_cities_links(self, html):\n",
    "        \"\"\"\n",
    "        Returns a list of the links to be scraped from the main webpage\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            html (str): HTML code of the original page\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            List of string values which are the links to the cities' pages\n",
    "\n",
    "        \"\"\"\n",
    "        # From the HTML file, we collect all the <td> tags that have a class = \"city-name\"\n",
    "        td_tags = html.find_all('td', {\"class\": \"city-name\"})\n",
    "        \n",
    "        # Then, for every <td> tag, we get the hyperlink in its <a> tag and add the extra currency reference\n",
    "        cities_links = [td.find('a').get('href') + \"?currency=EUR\" for td in td_tags]\n",
    "        return cities_links\n",
    "    \n",
    "    \n",
    "    def __get_ranking_pos(self, html):\n",
    "        \"\"\"\n",
    "        Returns the ranking position of a particular city\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            html (str): HTML code of the city's page\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Number of the position in string format\n",
    "\n",
    "        \"\"\"\n",
    "        # First, we get all <li> tags that have a class = \"key-point\"\n",
    "        li_tags = html.findAll('li', {'class': 'key-point'})\n",
    "        for li in li_tags:\n",
    "            # If the word \"World\" is in its text\n",
    "            if 'World' in li.text:\n",
    "                # We split the whole text by the word \"World\", keeping the side after it\n",
    "                text = li.text.split(\"World\", 1)[1].strip()\n",
    "                # From the remaining text we get the first 1, 2 or 3 digits that appear\n",
    "                pos = re.search(r'\\b(\\d{1,3})\\b', text).group(1)\n",
    "        return pos\n",
    "    \n",
    "    \n",
    "    def __scraping_single_city(self, url):\n",
    "        \"\"\"\n",
    "        Runs the Web Scraping of a particular city URL\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            url (str): link to the city's webpage\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # First we need to get the HTML file for this link\n",
    "        city_html = self.__get_HTML(url)\n",
    "        \n",
    "        # Then we get the Ranking position of the Country\n",
    "        pos = self.__get_ranking_pos(city_html)\n",
    "\n",
    "        # All the information is under <table> tag, with class: comparison single-city\n",
    "        table_tags = city_html.find('table', {'class': 'comparison single-city'})\n",
    "        \n",
    "        \n",
    "        # Setting the common variables for the scraping\n",
    "        current_category = \"\"\n",
    "        current_orig_currency = \"\"\n",
    "        current_country = \"\"\n",
    "        current_city = \"\"\n",
    "        current_state = \"\"\n",
    "        \n",
    "        # Setting the variable city, country and state (if exists)\n",
    "        location = city_html.find('span', {'class': 'city-2'}).text.split(\",\")\n",
    "        current_city = location[0].strip()\n",
    "        if len(location) == 1:\n",
    "            current_country = current_city\n",
    "        elif len(location)== 2:\n",
    "            current_country = location[1].strip()\n",
    "        else:\n",
    "            current_state = location[1].strip()\n",
    "            current_country = location[2].strip()\n",
    "        \n",
    "        \n",
    "        for i, tr in enumerate(table_tags.findAll(\"tr\")):\n",
    "            # If the tag has the class = \"categoryHeader\"\n",
    "            if \"categoryHeader\" in tr.get(\"class\", []):\n",
    "                # We get the first <th> tag \n",
    "                first_th_tag = tr.find(\"th\")\n",
    "                # And extract its text as the Category of the Dataset's row\n",
    "                current_category = first_th_tag.text\n",
    "            else:\n",
    "                # If not, it means it has <td> tags and we retrieve them all\n",
    "                td_tags = tr.findAll(\"td\")\n",
    "                \n",
    "                # Depending on how many <td> tags, we scrape different information\n",
    "                \n",
    "                # 2 tags means that we can retrive the currency being used in the city\n",
    "                if len(td_tags) == 2:\n",
    "                    currency = tr.find(\"td\")\n",
    "                    current_orig_currency = currency.text\n",
    "            \n",
    "                # 3 tags means we are scraping an european city that uses Euros as local currency, \n",
    "                # so the exchange common currency is going to be the same one\n",
    "                elif len(td_tags) == 3: \n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(current_country)\n",
    "                    self.dataset['City'].append(current_city)\n",
    "                    self.dataset['State'].append(current_state)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(\"EUR\")\n",
    "                    self.dataset['Original Currency Value'].append((td_tags[2].text.strip()))\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append((td_tags[2].text.strip()))\n",
    "                \n",
    "                # 4 tags means we are scraping a Country which currency is not Euros,\n",
    "                # so we scrape its own currency and its exchanged value in Euros\n",
    "                elif len(td_tags) == 4:\n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(current_country)\n",
    "                    self.dataset['City'].append(current_city)\n",
    "                    self.dataset['State'].append(current_state)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(current_orig_currency)\n",
    "                    self.dataset['Original Currency Value'].append((td_tags[2].text.strip()))\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append((td_tags[3].text.strip()))\n",
    "            \n",
    "\n",
    "    def __saving(self):\n",
    "        \"\"\"\n",
    "        Creates a Pandas Dataframe from the scraped data to save it as a CSV dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "\n",
    "        \"\"\"\n",
    "        # Let's create a Pandas Dataframe with the obtained dataset\n",
    "        expatistan_df = pd.DataFrame(self.dataset)\n",
    "        \n",
    "        # Now we save it as a CSV file with no index column\n",
    "        expatistan_df.to_csv(\"expatistan.csv\", index = False)\n",
    "        \n",
    "        print(\"\\nDataset saved as CSV file!\")\n",
    "        \n",
    "        \n",
    "    def scraping(self):\n",
    "        \"\"\"\n",
    "        Main method of the class that starts the Web Scraping of the main webpage\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "\n",
    "        \"\"\"\n",
    "        # 1. Get the original HTML file using Beautiful Soup\n",
    "        html = self.__get_HTML(self.original_url)\n",
    "        \n",
    "        # 2. Get all the Country links to scrape \n",
    "        cities_urls = self.__get_cities_links(html)\n",
    "        \n",
    "        # 3. For each country, let's scrape all the information\n",
    "        for url in cities_urls:\n",
    "            # From the link, we get the last \n",
    "            city = re.search(r\"living/([^/?]*)\", url).group(1)\n",
    "            city = re.sub(\"-\", \" \", city).title()\n",
    "            #print(\"Scraping city \" + city)\n",
    "            self.__scraping_single_city(url)\n",
    "\n",
    "        # 4. Finally, we save all the information ina CSV file\n",
    "        print(\"\\nScraping finished!\")\n",
    "        self.__saving()\n",
    "            \n",
    "\n",
    "    def __saving(self):\n",
    "        \"\"\"\n",
    "        Creates a Pandas Dataframe from the scraped data to save it as a CSV dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "\n",
    "        \"\"\"\n",
    "        # Let's create a Pandas Dataframe with the obtained dataset\n",
    "        cities_df = pd.DataFrame(self.dataset)\n",
    "        \n",
    "        # Now we save it as a CSV file with no index column\n",
    "        cities_df.to_csv(\"cost_of_living_cities.csv\", index = False)\n",
    "        \n",
    "        print(\"\\nDataset saved as CSV file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2884dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping city New York City\n",
      "Scraping city Grand Cayman\n",
      "Scraping city Hamilton Bermuda\n",
      "Scraping city Singapore\n",
      "Scraping city Maui\n",
      "Scraping city Zurich\n",
      "Scraping city Geneva\n",
      "Scraping city Basel\n",
      "Scraping city Lausanne\n",
      "Scraping city San Francisco\n",
      "Scraping city Miami\n",
      "Scraping city Lugano\n",
      "Scraping city Oakland California\n",
      "Scraping city London\n",
      "Scraping city Los Angeles\n",
      "Scraping city Washington D C\n",
      "Scraping city Honolulu\n",
      "Scraping city Denver\n",
      "Scraping city Hong Kong\n",
      "Scraping city Dublin\n",
      "Scraping city Dallas\n",
      "Scraping city Seattle\n",
      "Scraping city San Diego\n",
      "Scraping city San Jose California\n",
      "Scraping city Copenhagen\n",
      "Scraping city Austin\n",
      "Scraping city Philadelphia\n",
      "Scraping city Tampa\n",
      "Scraping city Nassau\n",
      "Scraping city Sydney\n",
      "Scraping city Vancouver\n",
      "Scraping city Atlanta\n",
      "Scraping city Amsterdam\n",
      "Scraping city Raleigh North Carolina\n",
      "Scraping city Toronto\n",
      "Scraping city Phoenix\n",
      "Scraping city Sacramento\n",
      "Scraping city Chicago\n",
      "Scraping city Auckland\n",
      "Scraping city Paris\n",
      "Scraping city Melbourne\n",
      "Scraping city Irvine\n",
      "Scraping city Cocoa Beach Florida\n",
      "Scraping city Minneapolis\n",
      "Scraping city Walnut Creek California\n",
      "Scraping city Trondheim\n",
      "Scraping city Orlando\n",
      "Scraping city Wellington\n",
      "Scraping city Adelaide\n",
      "Scraping city Tel Aviv\n",
      "Scraping city Salt Lake City\n",
      "Scraping city Doha\n",
      "Scraping city Luxembourg\n",
      "Scraping city Pittsburgh\n",
      "Scraping city Baltimore\n",
      "Scraping city Brisbane\n",
      "Scraping city Frankfurt Am Main\n",
      "Scraping city The Hague\n",
      "Scraping city Anchorage\n",
      "Scraping city Bristol\n",
      "Scraping city Newark\n",
      "Scraping city Calgary\n",
      "Scraping city Munich\n",
      "Scraping city Brighton And Hove\n",
      "Scraping city Manchester\n",
      "Scraping city Stockholm\n",
      "Scraping city Dubai\n",
      "Scraping city Houston\n",
      "Scraping city Helsinki\n",
      "Scraping city Hamburg\n",
      "Scraping city Perth Australia\n",
      "Scraping city Detroit\n",
      "Scraping city Rotterdam\n",
      "Scraping city Cork\n",
      "Scraping city Cologne\n",
      "Scraping city Edinburgh\n",
      "Scraping city Ottawa\n",
      "Scraping city New Orleans\n",
      "Scraping city San Antonio Texas\n",
      "Scraping city Montreal\n",
      "Scraping city Abu Dhabi\n",
      "Scraping city Nice\n",
      "Scraping city Jerusalem\n",
      "Scraping city Omaha\n",
      "Scraping city Oxford\n",
      "Scraping city Gothenburg\n",
      "Scraping city Edmonton\n",
      "Scraping city Aberdeen\n",
      "Scraping city Tokyo\n",
      "Scraping city Brussels\n",
      "Scraping city Santa Fe New Mexico\n",
      "Scraping city Cairns\n",
      "Scraping city Lyon\n",
      "Scraping city Milan\n",
      "Scraping city Hamilton New Zealand\n",
      "Scraping city Haifa\n",
      "Scraping city Vienna\n",
      "Scraping city Valletta\n",
      "Scraping city Plymouth\n",
      "Scraping city Newcastle Australia\n",
      "Scraping city Tucson\n",
      "Scraping city Barrie\n",
      "Scraping city Seoul\n",
      "Scraping city San Sebastian\n",
      "Scraping city Antwerp\n",
      "Scraping city Wollongong\n",
      "Scraping city Rome\n",
      "Scraping city Portsmouth\n",
      "Scraping city Darwin\n",
      "Scraping city Stuttgart\n",
      "Scraping city Modesto\n",
      "Scraping city Tallahassee\n",
      "Scraping city Montevideo\n",
      "Scraping city Bordeaux\n",
      "Scraping city Macao\n",
      "Scraping city Marseille\n",
      "Scraping city Stoke On Trent\n",
      "Scraping city Madrid\n",
      "Scraping city Ramallah\n",
      "Scraping city Winnipeg\n",
      "Scraping city Barcelona Spain\n",
      "Scraping city Osaka\n",
      "Scraping city Innsbruck\n",
      "Scraping city Florence\n",
      "Scraping city Nantes\n",
      "Scraping city Montpellier\n",
      "Scraping city Lille\n",
      "Scraping city Lisbon\n",
      "Scraping city Panama City\n",
      "Scraping city Ghent\n",
      "Scraping city Essen\n",
      "Scraping city Dortmund\n",
      "Scraping city Santo Domingo\n",
      "Scraping city Abbotsford\n",
      "Scraping city Freiburg\n",
      "Scraping city Venice\n",
      "Scraping city Varese\n",
      "Scraping city Quebec City\n",
      "Scraping city Athens Greece\n",
      "Scraping city San Jose Costa Rica\n",
      "Scraping city Bilbao\n",
      "Scraping city Bologna\n",
      "Scraping city Prague\n",
      "Scraping city Kuwait\n",
      "Scraping city Pamplona\n",
      "Scraping city Shanghai\n",
      "Scraping city Manama\n",
      "Scraping city Valencia Spain\n",
      "Scraping city Youngstown Ohio\n",
      "Scraping city Perugia\n",
      "Scraping city Malaga\n",
      "Scraping city Dresden\n",
      "Scraping city San Antonio California\n",
      "Scraping city Oporto\n",
      "Scraping city Taipei\n",
      "Scraping city Santiago Chile\n",
      "Scraping city Naples\n",
      "Scraping city Pisa\n",
      "Scraping city Pescara\n",
      "Scraping city Zaragoza\n",
      "Scraping city Oviedo\n",
      "Scraping city Turin\n",
      "Scraping city Santa Cruz De Tenerife\n",
      "Scraping city Amman\n",
      "Scraping city Cedar Rapids\n",
      "Scraping city Corunna\n",
      "Scraping city Monterrey\n",
      "Scraping city Alicante\n",
      "Scraping city Beijing\n",
      "Scraping city Santos Sao Paulo\n",
      "Scraping city Las Palmas De Gran Canaria\n",
      "Scraping city Seville\n",
      "Scraping city Funchal\n",
      "Scraping city Budapest\n",
      "Scraping city Sao Paulo\n",
      "Scraping city Mexico City\n",
      "Scraping city Moscow\n",
      "Scraping city Caracas\n",
      "Scraping city Shenzhen\n",
      "Scraping city Bangkok\n",
      "Scraping city Warsaw\n",
      "Scraping city Bratislava\n",
      "Scraping city New Guatemala\n",
      "Scraping city Almeria\n",
      "Scraping city Barquisimeto\n",
      "Scraping city Brasilia\n",
      "Scraping city Florianopolis\n",
      "Scraping city Belgrade\n",
      "Scraping city Rio De Janeiro\n",
      "Scraping city Belo Horizonte\n",
      "Scraping city Porto Alegre\n",
      "Scraping city Port Louis\n",
      "Scraping city Quito\n",
      "Scraping city Georgetown Guyana\n",
      "Scraping city Guadalajara\n",
      "Scraping city Bucharest\n",
      "Scraping city Suva Fiji\n",
      "Scraping city Queretaro\n",
      "Scraping city Johannesburg\n",
      "Scraping city Guayaquil\n",
      "Scraping city Manila\n",
      "Scraping city Lima\n",
      "Scraping city Vitoria Brazil\n",
      "Scraping city Phnom Penh\n",
      "Scraping city Jakarta\n",
      "Scraping city Chengdu\n",
      "Scraping city Kuala Lumpur\n",
      "Scraping city Nairobi\n",
      "Scraping city Ho Chi Minh City\n",
      "Scraping city Johor Bahru\n",
      "Scraping city Bogota\n",
      "Scraping city La Paz Bolivia\n",
      "Scraping city Yakutsk\n",
      "Scraping city Medellin\n",
      "Scraping city Plovdiv\n",
      "Scraping city Iasi\n",
      "Scraping city Mostar\n",
      "Scraping city Bandung\n",
      "Scraping city Mumbai\n",
      "Scraping city Burgas\n",
      "Scraping city Santiago De Cali\n",
      "Scraping city Zenica\n",
      "Scraping city Pristina\n",
      "Scraping city Targu Mures\n",
      "Scraping city Bangalore\n",
      "Scraping city Pune\n",
      "Scraping city Delhi\n",
      "Scraping city Yogyakarta\n",
      "Scraping city Nagpur\n",
      "Scraping city Irkutsk\n",
      "\n",
      "Scraping finished!\n",
      "\n",
      "Dataset saved as CSV file!\n"
     ]
    }
   ],
   "source": [
    "scraper = ExpatistanCityScraper()\n",
    "scraper.scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
