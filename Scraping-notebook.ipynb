{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3287e9ec",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.852 · Tipología y ciclo de vida de los datos · PRA1</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2022-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PRACTICA 1: Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a4bd0",
   "metadata": {},
   "source": [
    "## 1. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76ff3b",
   "metadata": {},
   "source": [
    "In this first section we are going to describe and explain the main libraries used in the notebook and why we need them:\n",
    "\n",
    "- `requests`:\n",
    "\tPython library that allows the user to send HTTP/1.1 requests easily (POST, GET, PUT, etc.). \n",
    "\tIt is beeing used to get the main content of the URLs used (GET request). \n",
    " \n",
    " \n",
    "- `builtwith`:\n",
    "\tPython library that detects the technology used by a website (Apache, JQuery, Wordpress),\n",
    "\tthe servers and several relevant information.  \n",
    "\tIt will be applied for detecting teh etchnology used for developing the web's design, as it will\n",
    "\tdefine the Web Scraping style that will be necessary to apply.\n",
    "    \n",
    "    \n",
    "- `beautifulsoup4`:\n",
    "\tPython library to scrape information from web pages easily through it HTML or XML file.\n",
    "\tIt is the main library for scraping all the information from the main URL.\n",
    "    \n",
    "    \n",
    "- `python-whois`:\n",
    "\tPython library that produces parsed WHOIS data for a given URL to be able to extract data for all\n",
    "\tthe popular TLDs (com, org, net, …). Also it enables the query of a WHOIS server directly instead of\n",
    "\tgoing through an intermediate web service.\n",
    "\tThis library allows us to know the owner of the webpage we want to scrape and to see if there are any\n",
    "\tscraping blockers.\n",
    "    \n",
    "    \n",
    "- `re`:\n",
    "\tPython library for creating regular expressions to search with.\n",
    "\tIt will helps us to extract the relevant information of the HTML's text.\n",
    "    \n",
    "    \n",
    "- `pandas`:\n",
    "\tPython package that provides fast, flexible, and expressive data structures designed to make working with\n",
    "\t\"relational\" or \"labeled\" data easy and intuitive.\n",
    "\tThis package will be fundamental to develop the final steps of this proyect by creating and exporting the\n",
    "\tfinal dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c5d14",
   "metadata": {},
   "source": [
    "### 1.1 Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482c749",
   "metadata": {},
   "source": [
    "If any of the forementioned libraries is not installed in the user's machine, this piece of code will installed them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01488804",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install builtwith\n",
    "!pip install beautifulsoup4\n",
    "!pip install python-whois"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af39f16",
   "metadata": {},
   "source": [
    "### 1.2 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16f648",
   "metadata": {},
   "source": [
    "Once the libraries are all installed in the machine, it is time to import them to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0049a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import builtwith\n",
    "import whois\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Python file with all the common function used in chapter 3.\n",
    "import utils     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ccb051",
   "metadata": {},
   "source": [
    "## 2. Information about the main URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the technologies of the webpage we want to scrap\n",
    "builtwith.parse('https://www.expatistan.com/cost-of-living/country/ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf6cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the content of the webpage\n",
    "page = requests.get(\"https://www.expatistan.com/cost-of-living/country/ranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998980b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's see the owner of the domain\n",
    "print(whois.whois(\"expatistan.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef7b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's analyse the structure of the html code\n",
    "soup = BeautifulSoup(page.content)\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390e4dd",
   "metadata": {},
   "source": [
    "## 3. Web Scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f34d6",
   "metadata": {},
   "source": [
    "### 3.1 Creation of the main class for the Country Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91237ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpatistanCountryScraper():\n",
    "    \"\"\"\n",
    "    Class to carry on the Web Scraping of the Country links from the original URL \n",
    "    www.expatistan.com\n",
    "    ...\n",
    "\n",
    "    Private Methods\n",
    "    ---------------\n",
    "        __init__(url):\n",
    "            Constructor of the class.\n",
    "        __scraping_single_country(self, url, country):\n",
    "            Extract the relevant information of a single Country link from its HTML file\n",
    "        __saving(self):\n",
    "            Creates a Pandas dataframe to save it as a CSV dataset\n",
    "    \n",
    "    Public Methods\n",
    "    --------------\n",
    "        scraping():\n",
    "            Main function of the class that starts the web scraping  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the Expatistan Web Scraper.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.original_url = \"https://www.expatistan.com/cost-of-living/country/ranking\"\n",
    "        \n",
    "        # The dataset, as a first instance, it is going to be stored as a JSON variable\n",
    "        self.dataset =  {\n",
    "            \"Ranking position\": [],\n",
    "            'Country': [],\n",
    "            'Category': [],\n",
    "            'Items': [],\n",
    "            'Original Currency': [],\n",
    "            'Original Currency Value': [],\n",
    "            'Exchanged Currency': [],\n",
    "            'Exchanged Currency Value': []\n",
    "        }\n",
    "         \n",
    "        \n",
    "    def __scraping_single_country(self, url, country):\n",
    "        \"\"\"\n",
    "        Runs the Web Scraping of a particular Country URL\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            link (str): link to the Country's webpage\n",
    "            country (str): name of the Country to be scraped\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # First we need to get the HTML file for this link\n",
    "        country_html = utils.get_HTML(url)\n",
    "        \n",
    "        # Then we get the Ranking position of the Country\n",
    "        pos = utils.get_ranking_pos(country_html, 2)\n",
    "\n",
    "        # All the information is under <table> tag, with class: comparison single-city\n",
    "        table_tags = country_html.find('table', {'class': 'comparison single-city'})\n",
    "\n",
    "        # Setting the common variables for the scraping\n",
    "        current_category = \"\"\n",
    "        current_orig_currency = \"\"\n",
    "        \n",
    "        # For every <tr> tag\n",
    "        for tr in table_tags.findAll(\"tr\"):\n",
    "            # If the tag has the class = \"categoryHeader\"\n",
    "            if \"categoryHeader\" in tr.get(\"class\", []):\n",
    "                # We get the first <th> tag \n",
    "                first_th_tag = tr.find(\"th\")\n",
    "                # And extract its text as the Category of the Dataset's row\n",
    "                current_category = first_th_tag.text\n",
    "                \n",
    "            else:\n",
    "                # If not, it means it has <td> tags and we retrieve them all\n",
    "                td_tags = tr.findAll(\"td\")\n",
    "                \n",
    "                # Depending on how many <td> tags, we scrape different information\n",
    "                # 2 tags means that we can retrive the currency being used in the Country\n",
    "                if len(td_tags) == 2:\n",
    "                    currency = tr.find(\"td\")\n",
    "                    current_orig_currency = currency.text\n",
    "\n",
    "                # 3 tags means we are scraping an european country that uses Euros as currency, \n",
    "                # so the exchange common currency is going to be the same one\n",
    "                elif len(td_tags) == 3: \n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(country)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(\"EUR\")\n",
    "                    self.dataset['Original Currency Value'].append((td_tags[2].text.strip()))\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append((td_tags[2].text.strip()))\n",
    "                \n",
    "                # 4 tags means we are scraping a Country which currency is not Euros,\n",
    "                # so we scrape its own currency and its exchanged value in Euros\n",
    "                elif len(td_tags) == 4:\n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(country)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(current_orig_currency)\n",
    "                    self.dataset['Original Currency Value'].append(td_tags[2].text.strip())\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append(td_tags[3].text.strip())\n",
    "                    \n",
    "        \n",
    "\n",
    "    def scraping(self):\n",
    "        \"\"\"\n",
    "        Main method of the class that starts the Web Scraping of the main webpage\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "\n",
    "        \"\"\"\n",
    "        # 1. Get the original HTML file using Beautiful Soup\n",
    "        html = utils.get_HTML(self.original_url)\n",
    "        \n",
    "        # 2. Get all the Country links to scrape \n",
    "        country_urls = utils.get_links(html, False)\n",
    "        \n",
    "        # 3. For each country, let's scrape all the information\n",
    "        print(\"Starting the Country's Web Scraping!\\n\")\n",
    "        for url in country_urls:\n",
    "            # From the link, we get the last \n",
    "            country = re.search(r\"country/([^/?]*)\", url).group(1)\n",
    "            country = re.sub(\"-\", \" \", country).title()\n",
    "            print(\"Scraping country: \" + country)\n",
    "            self.__scraping_single_country(url, country)\n",
    "\n",
    "        # 4. Finally, we save all the information ina CSV file\n",
    "        print(\"\\nScraping finished!\")\n",
    "        utils.saving(\"cost_of_living_countries.csv\", self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c38f2f",
   "metadata": {},
   "source": [
    "#### 3.1.1 Scraping of Cost of Living per Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78db1640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the Country's Web Scraping!\n",
      "\n",
      "Scraping country: Bermuda\n",
      "Scraping country: Singapore\n",
      "Scraping country: Switzerland\n",
      "Scraping country: Cayman Islands\n",
      "Scraping country: Bahamas\n",
      "Scraping country: Hong Kong\n",
      "Scraping country: Ireland\n",
      "Scraping country: Denmark\n",
      "Scraping country: United States\n",
      "Scraping country: Norway\n",
      "Scraping country: Netherlands\n",
      "Scraping country: Luxembourg\n",
      "Scraping country: Australia\n",
      "Scraping country: New Zealand\n",
      "Scraping country: Qatar\n",
      "Scraping country: United Kingdom\n",
      "Scraping country: Israel\n",
      "Scraping country: Canada\n",
      "Scraping country: Germany\n",
      "Scraping country: Finland\n",
      "Scraping country: Belgium\n",
      "Scraping country: Sweden\n",
      "Scraping country: Austria\n",
      "Scraping country: France\n",
      "Scraping country: Japan\n",
      "Scraping country: United Arab Emirates\n",
      "Scraping country: Malta\n",
      "Scraping country: South Korea\n",
      "Scraping country: Macao\n",
      "Scraping country: Uruguay\n",
      "Scraping country: Italy\n",
      "Scraping country: Panama\n",
      "Scraping country: Dominican Republic\n",
      "Scraping country: Kuwait\n",
      "Scraping country: Palestinian Territory\n",
      "Scraping country: Costa Rica\n",
      "Scraping country: Portugal\n",
      "Scraping country: Bahrain\n",
      "Scraping country: Spain\n",
      "Scraping country: Czech Republic\n",
      "Scraping country: Slovakia\n",
      "Scraping country: Greece\n",
      "Scraping country: China\n",
      "Scraping country: Jordan\n",
      "Scraping country: Chile\n",
      "Scraping country: Taiwan\n",
      "Scraping country: Hungary\n",
      "Scraping country: Thailand\n",
      "Scraping country: Venezuela\n",
      "Scraping country: Poland\n",
      "Scraping country: Serbia\n",
      "Scraping country: Guatemala\n",
      "Scraping country: Bosnia And Herzegovina\n",
      "Scraping country: Mexico\n",
      "Scraping country: Brazil\n",
      "Scraping country: Guyana\n",
      "Scraping country: Cambodia\n",
      "Scraping country: Mauritius\n",
      "Scraping country: Bulgaria\n",
      "Scraping country: Romania\n",
      "Scraping country: Ecuador\n",
      "Scraping country: Vietnam\n",
      "Scraping country: South Africa\n",
      "Scraping country: Fiji\n",
      "Scraping country: Philippines\n",
      "Scraping country: Indonesia\n",
      "Scraping country: Malaysia\n",
      "Scraping country: Russia\n",
      "Scraping country: Peru\n",
      "Scraping country: Kenya\n",
      "Scraping country: Bolivia\n",
      "Scraping country: Colombia\n",
      "Scraping country: Kosovo\n",
      "Scraping country: India\n",
      "\n",
      "Scraping finished!\n",
      "\n",
      "Dataset cost_of_living_countries.csv saved as CSV file!\n"
     ]
    }
   ],
   "source": [
    "scraper = ExpatistanCountryScraper()\n",
    "scraper.scraping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946f8b9",
   "metadata": {},
   "source": [
    "### 3.2 Creation of the main class for the City Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7cc1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpatistanCityScraper():\n",
    "    \"\"\"\n",
    "    Class to carry on the Web Scraping of the cities links from the original URL \n",
    "    www.expatistan.com\n",
    "    ...\n",
    "\n",
    "    Private Methods\n",
    "    ---------------\n",
    "        __init__(url):\n",
    "            Constructor of the class.\n",
    "        __scraping_single_country(self, url, country):\n",
    "            Extract the relevant information of a single Country link from its HTML file\n",
    "        __saving(self):\n",
    "            Creates a Pandas dataframe to save it as a CSV dataset\n",
    "    \n",
    "    Public Methods\n",
    "    --------------\n",
    "        scraping():\n",
    "            Main function of the class that starts the web scraping  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the Expatistan Web Scraper.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.original_url = \"https://www.expatistan.com/cost-of-living/index?ranking=1\"\n",
    "        \n",
    "        # The dataset, as a first instance, it is going to be stored as a JSON variable\n",
    "        self.dataset =  {\n",
    "            \"Ranking position\": [],\n",
    "            'Country': [],\n",
    "            'City': [],\n",
    "            'State': [],\n",
    "            'Category': [],\n",
    "            'Items': [],\n",
    "            'Original Currency': [],\n",
    "            'Original Currency Value': [],\n",
    "            'Exchanged Currency': [],\n",
    "            'Exchanged Currency Value': []\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def __scraping_single_city(self, url):\n",
    "        \"\"\"\n",
    "        Runs the Web Scraping of a particular city URL\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "            url (str): link to the city's webpage\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # First we need to get the HTML file for this link\n",
    "        city_html = utils.get_HTML(url)\n",
    "        \n",
    "        # Then we get the Ranking position of the Country\n",
    "        pos = utils.get_ranking_pos(city_html, 3)\n",
    "\n",
    "        # All the information is under <table> tag, with class: comparison single-city\n",
    "        table_tags = city_html.find('table', {'class': 'comparison single-city'})\n",
    "        \n",
    "        \n",
    "        # Setting the common variables for the scraping\n",
    "        current_category = \"\"\n",
    "        current_orig_currency = \"\"\n",
    "        current_country = \"\"\n",
    "        current_city = \"\"\n",
    "        current_state = \"\"\n",
    "        \n",
    "        # Setting the variable city, country and state (if exists)\n",
    "        location = city_html.find('span', {'class': 'city-2'}).text.split(\",\")\n",
    "        current_city = location[0].strip()\n",
    "        if len(location) == 1:\n",
    "            current_state = \"No state\"\n",
    "            current_country = current_city\n",
    "        elif len(location) == 2:\n",
    "            current_state = \"No state\"\n",
    "            current_country = location[1].strip()\n",
    "        else:\n",
    "            current_state = location[1].strip()\n",
    "            current_country = location[2].strip()\n",
    "        \n",
    "        \n",
    "        for tr in table_tags.findAll(\"tr\"):\n",
    "            # If the tag has the class = \"categoryHeader\"\n",
    "            if \"categoryHeader\" in tr.get(\"class\", []):\n",
    "                # We get the first <th> tag \n",
    "                first_th_tag = tr.find(\"th\")\n",
    "                # And extract its text as the Category of the Dataset's row\n",
    "                current_category = first_th_tag.text\n",
    "                \n",
    "            else:\n",
    "                # If not, it means it has <td> tags and we retrieve them all\n",
    "                td_tags = tr.findAll(\"td\")\n",
    "                \n",
    "                # Depending on how many <td> tags, we scrape different information\n",
    "                # 2 tags means that we can retrive the currency being used in the city\n",
    "                if len(td_tags) == 2:\n",
    "                    currency = tr.find(\"td\")\n",
    "                    current_orig_currency = currency.text\n",
    "            \n",
    "                # 3 tags means we are scraping an european city that uses Euros as local currency, \n",
    "                # so the exchange common currency is going to be the same one\n",
    "                elif len(td_tags) == 3: \n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(current_country)\n",
    "                    self.dataset['City'].append(current_city)\n",
    "                    self.dataset['State'].append(current_state)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(\"EUR\")\n",
    "                    self.dataset['Original Currency Value'].append(\"Not defined\"\n",
    "                                                                   if \"-\" in td_tags[2].text.strip()\n",
    "                                                                   else td_tags[2].text.strip())\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append(\"Not defined\"\n",
    "                                                                   if \"-\" in td_tags[2].text.strip()\n",
    "                                                                   else td_tags[2].text.strip())\n",
    "                \n",
    "                # 4 tags means we are scraping a Country which currency is not Euros,\n",
    "                # so we scrape its own currency and its exchanged value in Euros\n",
    "                elif len(td_tags) == 4:\n",
    "                    self.dataset['Ranking position'].append(pos)\n",
    "                    self.dataset['Country'].append(current_country)\n",
    "                    self.dataset['City'].append(current_city)\n",
    "                    self.dataset['State'].append(current_state)\n",
    "                    self.dataset['Category'].append(current_category)\n",
    "                    self.dataset['Items'].append(td_tags[1].text.strip())\n",
    "                    self.dataset['Original Currency'].append(current_orig_currency)\n",
    "                    self.dataset['Original Currency Value'].append(\"Not defined\"\n",
    "                                                                   if \"-\" in td_tags[2].text.strip()\n",
    "                                                                   else td_tags[2].text.strip())\n",
    "                    self.dataset['Exchanged Currency'].append(\"EUR\")\n",
    "                    self.dataset['Exchanged Currency Value'].append(\"Not defined\"\n",
    "                                                                   if \"-\" in td_tags[3].text.strip()\n",
    "                                                                   else td_tags[3].text.strip())\n",
    "        \n",
    "    def scraping(self):\n",
    "        \"\"\"\n",
    "        Main method of the class that starts the Web Scraping of the main webpage\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            self (class): Instance of the class that is invoking the method\n",
    "\n",
    "        \"\"\"\n",
    "        # 1. Get the original HTML file using Beautiful Soup\n",
    "        html = utils.get_HTML(self.original_url)\n",
    "        \n",
    "        # 2. Get all the Country links to scrape \n",
    "        cities_urls = utils.get_links(html, True)\n",
    "        \n",
    "        # 3. For each country, let's scrape all the information\n",
    "        print(\"Starting the City's Web Scraping!\\n\")\n",
    "        for url in cities_urls:\n",
    "            # From the link, we get the last \n",
    "            city = re.search(r\"living/([^/?]*)\", url).group(1)\n",
    "            city = re.sub(\"-\", \" \", city).title()\n",
    "            print(\"Scraping city: \" + city)\n",
    "            self.__scraping_single_city(url)\n",
    "\n",
    "        # 4. Finally, we save all the information ina CSV file\n",
    "        print(\"\\nScraping finished!\")\n",
    "        utils.saving(\"cost_of_living_cities.csv\", self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb0e4d",
   "metadata": {},
   "source": [
    "#### 3.2.1 Scraping of Cost of Living per City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb2884dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the City's Web Scraping!\n",
      "\n",
      "Scraping city: New York City\n",
      "Scraping city: Grand Cayman\n",
      "Scraping city: Hamilton Bermuda\n",
      "Scraping city: Singapore\n",
      "Scraping city: Maui\n",
      "Scraping city: Zurich\n",
      "Scraping city: Geneva\n",
      "Scraping city: Basel\n",
      "Scraping city: Lausanne\n",
      "Scraping city: San Francisco\n",
      "Scraping city: Miami\n",
      "Scraping city: Lugano\n",
      "Scraping city: Oakland California\n",
      "Scraping city: London\n",
      "Scraping city: Los Angeles\n",
      "Scraping city: Washington D C\n",
      "Scraping city: Honolulu\n",
      "Scraping city: Denver\n",
      "Scraping city: Hong Kong\n",
      "Scraping city: Dublin\n",
      "Scraping city: Dallas\n",
      "Scraping city: Seattle\n",
      "Scraping city: San Diego\n",
      "Scraping city: San Jose California\n",
      "Scraping city: Copenhagen\n",
      "Scraping city: Austin\n",
      "Scraping city: Philadelphia\n",
      "Scraping city: Tampa\n",
      "Scraping city: Nassau\n",
      "Scraping city: Sydney\n",
      "Scraping city: Vancouver\n",
      "Scraping city: Atlanta\n",
      "Scraping city: Amsterdam\n",
      "Scraping city: Raleigh North Carolina\n",
      "Scraping city: Toronto\n",
      "Scraping city: Phoenix\n",
      "Scraping city: Sacramento\n",
      "Scraping city: Chicago\n",
      "Scraping city: Auckland\n",
      "Scraping city: Paris\n",
      "Scraping city: Melbourne\n",
      "Scraping city: Irvine\n",
      "Scraping city: Cocoa Beach Florida\n",
      "Scraping city: Minneapolis\n",
      "Scraping city: Walnut Creek California\n",
      "Scraping city: Trondheim\n",
      "Scraping city: Orlando\n",
      "Scraping city: Wellington\n",
      "Scraping city: Adelaide\n",
      "Scraping city: Tel Aviv\n",
      "Scraping city: Salt Lake City\n",
      "Scraping city: Doha\n",
      "Scraping city: Luxembourg\n",
      "Scraping city: Pittsburgh\n",
      "Scraping city: Baltimore\n",
      "Scraping city: Brisbane\n",
      "Scraping city: Frankfurt Am Main\n",
      "Scraping city: The Hague\n",
      "Scraping city: Anchorage\n",
      "Scraping city: Bristol\n",
      "Scraping city: Newark\n",
      "Scraping city: Calgary\n",
      "Scraping city: Munich\n",
      "Scraping city: Brighton And Hove\n",
      "Scraping city: Manchester\n",
      "Scraping city: Stockholm\n",
      "Scraping city: Dubai\n",
      "Scraping city: Houston\n",
      "Scraping city: Helsinki\n",
      "Scraping city: Hamburg\n",
      "Scraping city: Perth Australia\n",
      "Scraping city: Detroit\n",
      "Scraping city: Rotterdam\n",
      "Scraping city: Cork\n",
      "Scraping city: Cologne\n",
      "Scraping city: Edinburgh\n",
      "Scraping city: Ottawa\n",
      "Scraping city: New Orleans\n",
      "Scraping city: San Antonio Texas\n",
      "Scraping city: Montreal\n",
      "Scraping city: Abu Dhabi\n",
      "Scraping city: Nice\n",
      "Scraping city: Jerusalem\n",
      "Scraping city: Omaha\n",
      "Scraping city: Oxford\n",
      "Scraping city: Gothenburg\n",
      "Scraping city: Edmonton\n",
      "Scraping city: Aberdeen\n",
      "Scraping city: Tokyo\n",
      "Scraping city: Brussels\n",
      "Scraping city: Santa Fe New Mexico\n",
      "Scraping city: Cairns\n",
      "Scraping city: Lyon\n",
      "Scraping city: Milan\n",
      "Scraping city: Hamilton New Zealand\n",
      "Scraping city: Haifa\n",
      "Scraping city: Vienna\n",
      "Scraping city: Valletta\n",
      "Scraping city: Plymouth\n",
      "Scraping city: Newcastle Australia\n",
      "Scraping city: Tucson\n",
      "Scraping city: Barrie\n",
      "Scraping city: Seoul\n",
      "Scraping city: San Sebastian\n",
      "Scraping city: Antwerp\n",
      "Scraping city: Wollongong\n",
      "Scraping city: Rome\n",
      "Scraping city: Portsmouth\n",
      "Scraping city: Darwin\n",
      "Scraping city: Stuttgart\n",
      "Scraping city: Modesto\n",
      "Scraping city: Tallahassee\n",
      "Scraping city: Montevideo\n",
      "Scraping city: Bordeaux\n",
      "Scraping city: Macao\n",
      "Scraping city: Marseille\n",
      "Scraping city: Stoke On Trent\n",
      "Scraping city: Madrid\n",
      "Scraping city: Ramallah\n",
      "Scraping city: Winnipeg\n",
      "Scraping city: Barcelona Spain\n",
      "Scraping city: Osaka\n",
      "Scraping city: Innsbruck\n",
      "Scraping city: Florence\n",
      "Scraping city: Nantes\n",
      "Scraping city: Montpellier\n",
      "Scraping city: Lille\n",
      "Scraping city: Lisbon\n",
      "Scraping city: Panama City\n",
      "Scraping city: Ghent\n",
      "Scraping city: Essen\n",
      "Scraping city: Dortmund\n",
      "Scraping city: Santo Domingo\n",
      "Scraping city: Abbotsford\n",
      "Scraping city: Freiburg\n",
      "Scraping city: Venice\n",
      "Scraping city: Varese\n",
      "Scraping city: Quebec City\n",
      "Scraping city: Athens Greece\n",
      "Scraping city: San Jose Costa Rica\n",
      "Scraping city: Bilbao\n",
      "Scraping city: Bologna\n",
      "Scraping city: Prague\n",
      "Scraping city: Kuwait\n",
      "Scraping city: Pamplona\n",
      "Scraping city: Shanghai\n",
      "Scraping city: Manama\n",
      "Scraping city: Valencia Spain\n",
      "Scraping city: Youngstown Ohio\n",
      "Scraping city: Perugia\n",
      "Scraping city: Malaga\n",
      "Scraping city: Dresden\n",
      "Scraping city: San Antonio California\n",
      "Scraping city: Oporto\n",
      "Scraping city: Taipei\n",
      "Scraping city: Santiago Chile\n",
      "Scraping city: Naples\n",
      "Scraping city: Pisa\n",
      "Scraping city: Pescara\n",
      "Scraping city: Zaragoza\n",
      "Scraping city: Oviedo\n",
      "Scraping city: Turin\n",
      "Scraping city: Santa Cruz De Tenerife\n",
      "Scraping city: Amman\n",
      "Scraping city: Cedar Rapids\n",
      "Scraping city: Corunna\n",
      "Scraping city: Monterrey\n",
      "Scraping city: Alicante\n",
      "Scraping city: Beijing\n",
      "Scraping city: Santos Sao Paulo\n",
      "Scraping city: Las Palmas De Gran Canaria\n",
      "Scraping city: Seville\n",
      "Scraping city: Funchal\n",
      "Scraping city: Budapest\n",
      "Scraping city: Sao Paulo\n",
      "Scraping city: Mexico City\n",
      "Scraping city: Moscow\n",
      "Scraping city: Caracas\n",
      "Scraping city: Shenzhen\n",
      "Scraping city: Bangkok\n",
      "Scraping city: Warsaw\n",
      "Scraping city: Bratislava\n",
      "Scraping city: New Guatemala\n",
      "Scraping city: Almeria\n",
      "Scraping city: Barquisimeto\n",
      "Scraping city: Brasilia\n",
      "Scraping city: Florianopolis\n",
      "Scraping city: Belgrade\n",
      "Scraping city: Rio De Janeiro\n",
      "Scraping city: Belo Horizonte\n",
      "Scraping city: Porto Alegre\n",
      "Scraping city: Port Louis\n",
      "Scraping city: Quito\n",
      "Scraping city: Georgetown Guyana\n",
      "Scraping city: Guadalajara\n",
      "Scraping city: Bucharest\n",
      "Scraping city: Suva Fiji\n",
      "Scraping city: Queretaro\n",
      "Scraping city: Johannesburg\n",
      "Scraping city: Guayaquil\n",
      "Scraping city: Manila\n",
      "Scraping city: Lima\n",
      "Scraping city: Vitoria Brazil\n",
      "Scraping city: Phnom Penh\n",
      "Scraping city: Jakarta\n",
      "Scraping city: Chengdu\n",
      "Scraping city: Kuala Lumpur\n",
      "Scraping city: Nairobi\n",
      "Scraping city: Ho Chi Minh City\n",
      "Scraping city: Johor Bahru\n",
      "Scraping city: Bogota\n",
      "Scraping city: La Paz Bolivia\n",
      "Scraping city: Yakutsk\n",
      "Scraping city: Medellin\n",
      "Scraping city: Plovdiv\n",
      "Scraping city: Iasi\n",
      "Scraping city: Mostar\n",
      "Scraping city: Bandung\n",
      "Scraping city: Mumbai\n",
      "Scraping city: Burgas\n",
      "Scraping city: Santiago De Cali\n",
      "Scraping city: Zenica\n",
      "Scraping city: Pristina\n",
      "Scraping city: Targu Mures\n",
      "Scraping city: Bangalore\n",
      "Scraping city: Pune\n",
      "Scraping city: Delhi\n",
      "Scraping city: Yogyakarta\n",
      "Scraping city: Nagpur\n",
      "Scraping city: Irkutsk\n",
      "\n",
      "Scraping finished!\n",
      "\n",
      "Dataset cost_of_living_cities.csv saved as CSV file!\n"
     ]
    }
   ],
   "source": [
    "scraper = ExpatistanCityScraper()\n",
    "scraper.scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5535d8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
